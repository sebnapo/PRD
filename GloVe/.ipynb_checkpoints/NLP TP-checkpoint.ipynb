{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- IMPORT SECTION ---- #\n",
    "import gensim, logging\n",
    "import os\n",
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from array import array\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import treetaggerwrapper\n",
    "from gensim.test.utils import datapath\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<HEADLINE>': {'$10,000': 1, 'Gold?': 1, '<AUTHOR': 1}, '$10,000': {'Gold?': 1, '<AUTHOR': 1, 'name=\"Kenneth': 1}, 'Gold?': {'<AUTHOR': 1, 'name=\"Kenneth': 1, 'Rogoff\">': 1}, '<AUTHOR': {'name=\"Kenneth': 1, 'Rogoff\">': 1, '<P>': 1}, 'name=\"Kenneth': {'Rogoff\">': 1, '<P>': 1, 'SAN': 1}, 'Rogoff\">': {'<P>': 1, 'SAN': 1, 'FRANCISCO': 1}, '<P>': {'SAN': 1, 'FRANCISCO': 1, '–': 1, 'Wouldn’t': 1, 'you': 2, 'know': 1, 'One': 2, 'successful': 1, 'gold': 1, 'Admittedly,': 1, 'getting': 1, 'to': 1, 'answer,': 1, 'of': 1, 'Sure,': 1, 'some': 1, 'might': 1, 'Even': 1, 'so,': 1, 'the': 2, 'There': 1, 'is': 1, 'probably': 1, 'In': 1, 'my': 1, 'view,': 1, 'At': 1, 'same': 1, 'So,': 1, 'yes,': 1, 'there': 1, 'Indeed,': 1, 'another': 1, 'critical': 1, 'Most': 1, 'economic': 1, 'research': 1, 'If': 1, 'are': 1, 'Of': 1, 'course,': 1, 'such': 1}, 'SAN': {'FRANCISCO': 1, '–': 1, 'It': 1}, 'FRANCISCO': {'–': 1, 'It': 1, 'has': 1}, '–': {'It': 1, 'has': 1, 'never': 1, 'and': 1, 'also': 1, 'a': 1, 'to': 1, 'this': 1, 'argument.': 1}, 'It': {'has': 1, 'never': 1, 'been': 1, 'is': 1, 'therefore': 1, 'dangerous': 1}, 'has': {'never': 1, 'been': 3, 'easy': 1, 'moved': 1, 'up': 1, 'still': 1, 'climbed': 1, 'above': 1, '10,000.': 1, 'crossed': 1, 'the': 1, 'magic': 1, 'partly': 1, 'driven': 1, 'naturally': 1, 'grown.': 1, '<P>': 1, 'sustaining': 1, 'high': 1, 'had': 1, 'a': 1, 'great': 1}, 'never': {'been': 1, 'easy': 1, 'to': 1}, 'been': {'easy': 1, 'to': 1, 'have': 1, 'partly': 1, 'driven': 1, 'by': 1, 'sustaining': 1, 'high': 1, 'gold': 1}, 'easy': {'to': 1, 'have': 1, 'a': 1}, 'to': {'have': 1, 'a': 3, 'rational': 1, 'me': 1, 'that': 1, 'stock': 1, 'much': 1, 'higher': 2, 'the': 2, 'printing': 1, 'press.': 1, 'worry': 1, 'about': 1, 'whether': 1, 'non-indexed': 1, 'debt,': 1, 'precisely': 1, 'trade': 1, 'and': 1, 'speculate': 2, 'this': 1, 'argument.': 1, 'After': 1, 'transform': 1, 'base': 1, 'metals': 1, 'justify': 1, 'today’s': 1, 'high': 1, 'accumulate': 1, 'gold': 2, 'reserves,': 1, 'what': 1, 'extent': 1, 'they': 1, 'support': 1, 'prices': 1, 'global': 1, 'interest-rate': 1, 'movements.': 1, 'store.': 1, 'Today,': 1, 'with': 2, 'in': 1, 'predict': 1, 'over': 1, 'medium': 1, 'term,': 1, 'extrapolate': 1, 'from': 1, 'short-term': 1, 'hold': 1, 'modest': 1, 'reconcile.': 1}, 'have': {'a': 1, 'rational': 1, 'conversation': 1, 'argued': 1, 'instead': 1, 'that': 1, 'little': 1, 'influence': 1, 'on': 1}, 'a': {'rational': 1, 'conversation': 1, 'about': 1, 'record-high': 1, '$1,300': 1, 'recently.': 1, 'more': 2, 'than': 1, 'a': 2, 'decade': 1, 'before': 1, 'the': 1, 'much': 1, 'higher': 1, 'price': 1, '“freak': 1, 'peak”': 1, 'during': 1, 'period': 1, 'of': 5, 'heightened': 1, 'complete': 1, 'collapse': 1, 'rudderless': 1, 'fiscal': 1, 'policy,': 1, 'populist': 1, 'administration': 1, 'might': 1, 'better': 1, 'and': 1, 'certain': 1, 'degree': 1, 'diversification': 1, 'play': 1, 'away': 1, 'great': 1, 'run,': 1, 'but': 1, 'couple': 1, 'years': 1, 'high-net-worth': 1, 'investor,': 1, 'sovereign': 1, 'wealth': 1, 'fund,': 1, 'central': 1, 'bank,': 1, 'it': 1, 'modest': 1, 'proportion': 1, 'hedge': 1, 'against': 1, 'extreme': 1, 'very': 1, 'risky': 1, 'bet': 1}, 'rational': {'conversation': 1, 'about': 1, 'the': 1}, 'conversation': {'about': 1, 'the': 1, 'value': 1}, 'about': {'the': 2, 'value': 1, 'of': 1, 'that,': 1, 'gold': 1, 'might': 1, 'whether': 1, 'government': 1}, 'the': {'value': 1, 'of': 12, 'gold.': 1, 'last': 1, 'decade,': 1, 'it': 2, 'price': 3, 'gold': 1, 'was': 1, 'inevitably': 1, 'Dow': 1, 'Jones': 1, 'index': 2, '1,000': 1, 'mark': 1, 'in': 1, 'early': 1, '1980’s.': 1, 'Since': 1, 'has': 1, 'climbed': 1, 'magic': 1, '$1,000': 1, 'barrier,': 1, 'leap': 1, 'imagination': 1, 'all-time': 1, 'high': 1, 'US': 1, 'dollar.': 1, 'With': 1, 'printing': 1, 'press.': 1, 'And': 1, 'most': 2, 'reliable': 1, 'hedge.': 1, 'government': 1, 'will': 1, 'honor': 1, 'history': 1, 'financial': 1, 'United': 1, 'States': 1, 'abrogated': 1, 'Great': 1, 'Depression': 1, '1930’s.': 1, 'So': 1, 'fact': 1, 'that': 1, 'very': 1, 'development': 1, 'new': 1, 'powerful': 1, 'argument': 1, 'dramatic': 1, 'emergence': 1, 'Middle': 1, 'East': 1, 'into': 1, 'global': 1, 'economy.': 1, 'As': 1, 'scarce': 1, 'same': 1, 'time,': 1, 'emerging-market': 1, 'euro': 1, 'looking': 1, 'less': 1, 'dollar,': 1, 'gold’s': 1, 'appeal': 1, 'future.': 1, '<P>': 1, 'Indeed,': 1, 'short': 1, 'to': 1, 'medium': 1, 'odds': 1, 'gains': 1, 'wake': 1, 'an': 1, 'alchemists': 1, 'yore': 1}, 'value': {'of': 1, 'gold.': 1, 'Lately,': 1, 'might': 1, 'be': 1, 'inflated': 1}, 'of': {'gold.': 1, 'Lately,': 1, 'with': 1, 'gold': 3, 'has': 1, 'moved': 1, 'imagination': 1, 'that': 1, 'it': 1, 'January': 1, '1980.': 1, 'Back': 1, 'heightened': 1, 'geo-political': 1, 'instability.': 1, 'course,': 1, 'is': 2, 'a': 1, 'the': 3, 'US': 1, 'dollar.': 1, 'financial': 2, 'crises,': 1, 'This': 1, '1930’s.': 1, 'So': 1, 'new': 2, 'instruments': 1, 'irony': 1, '–': 1, 'to': 1, 'worth': 1, 'dramatically': 1, 'Asia,': 1, 'Latin': 1, 'America,': 1, 'consumers': 1, 'gain': 1, 'scarce': 1, 'commodities.': 1, '<P>': 3, 'investing': 1, 'in': 2, 'bonds.': 1, 'gains': 1, 'and': 1, 'losses': 1, 'years': 1, 'ago.': 1, 'your': 1, 'portfolio': 1, 'an': 1, 'extraordinary': 1, 'run-up': 1, 'us.': 1, 'Of': 1, 'yore': 1, 'remains': 1, 'true': 1}, 'gold.': {'Lately,': 1, 'with': 1, 'gold': 2, 'But': 1, 'bugs': 1, '<P>': 1, 'There': 1, 'is': 1, 'Wouldn’t': 1, 'it': 1, 'be': 1}, 'Lately,': {'with': 1, 'gold': 1, 'prices': 1}, 'with': {'gold': 1, 'prices': 1, 'up': 1, 'interest': 1, 'rates': 1, 'near': 1, 'the': 1, 'odds': 1, 'of': 1}, 'gold': {'prices': 6, 'up': 2, 'more': 1, 'has': 3, 'moved': 1, 'bugs': 2, 'were': 1, 'arguing': 1, 'could': 2, 'be': 2, 'headed': 1, 'investor': 1, 'recently': 1, 'explained': 1, 'crossed': 1, 'the': 2, 'is': 2, 'not': 1, 'quite': 1, 'hit': 1, '$850,': 1, 'or': 1, 'prices.': 1, 'So': 1, 'what': 1, 'from': 1, 'here?': 1, 'might': 2, 'indeed': 1, 'are': 4, 'right': 1, 'being': 1, 'worth': 1, 'dramatically': 1, 'more?': 1, 'dramatic': 1, 'reserves,': 1, 'which': 1, 'they': 1, 'price,': 1, 'although': 1, 'it': 1, 'prove': 1, 'pays': 1, 'no': 1, 'interest': 1, 'instead': 1, 'of': 1, 'investing': 1, 'plummet.': 1, 'very': 1, 'had': 1, 'a': 2, 'as': 1, 'hedge': 1, 'and': 1, 'reason': 1}, 'prices': {'up': 1, 'more': 1, 'than': 1, 'even': 1, 'hit': 1, 'a': 3, 'languished': 1, 'for': 1, 'from': 1, 'here?': 1, '<P>': 2, 'are': 3, 'being': 1, 'driven': 1, 'in': 1, 'the': 1, 'future.': 1, 'might': 1, 'prove': 1, 'far': 1, 'extremely': 1, 'sensitive': 1, 'could': 1, 'plummet.': 1, 'very': 1, 'difficult': 1, 'until': 1, 'couple': 1}, 'up': {'more': 1, 'than': 1, '300%': 1, 'still': 1, 'further.': 1, 'Gold': 1, 'the': 1, 'price': 1, 'of': 1}, 'more': {'than': 4, '300%': 1, 'over': 1, 'a': 1, 'decade': 1, 'double': 1, 'very': 1, 'direct': 1, 'inflation': 1, 'hedge': 1, 'extreme': 1, 'circumstances.': 1, 'In': 1, 'debatable': 1, 'whether': 1, 'and': 1, 'ephemeral': 1, 'globalization.': 1}, 'than': {'300%': 1, 'over': 1, 'the': 1, 'ever.': 1, 'Just': 1, 'last': 1, 'that.': 1, '<P>': 1, 'One': 1, 'a': 1, 'decade': 1, 'before': 1, 'double': 1, 'very': 1, 'long-term,': 1, 'gold.': 1, 'But': 1, 'gold': 1, 'do': 1, 'rich-country': 1, 'central': 1, 'globalization.': 1, 'Gold': 1, 'prices': 1}, '300%': {'over': 1, 'the': 1, 'last': 1}, 'over': {'the': 2, 'last': 1, 'decade,': 1, '$2,000': 1, 'in': 1, 'today’s': 1, 'short': 1, 'to': 1}, 'last': {'decade,': 1, 'it': 1, 'is': 1, 'December,': 1, 'fellow': 1, 'economists': 1}, 'decade,': {'it': 1, 'is': 1, 'harder': 1}, 'it': {'is': 3, 'harder': 1, 'than': 1, 'increase': 1, 'ten-fold,': 1, 'too?': 1, 'seems.': 1, 'After': 1, 'adjusting': 1, 'can': 1, 'happen': 1, 'anywhere.': 1, 'probable,': 1, 'so': 1, 'one': 1, 'easier': 1, 'to': 1, 'trade': 1, 'be': 1, 'paradoxical,': 1, 'then,': 1, 'far': 1, 'more': 1, 'relatively': 1, 'cheap': 1, 'makes': 1, 'perfect': 1, 'sense': 1, 'remains': 1, 'a': 1, 'very': 1}, 'is': {'harder': 1, 'than': 2, 'ever.': 1, 'not': 2, 'quite': 1, 'the': 3, 'nowhere': 1, 'near': 1, 'probably': 2, 'more': 2, 'a': 1, 'complete': 1, 'collapse': 1, 'Different,': 1, 'cash-strapped': 1, 'governments': 1, 'possible': 1, 'does': 1, 'some': 1, 'slight': 1, 'dramatic': 1, 'emergence': 1, 'far': 1, 'debatable': 1, 'relatively': 1, 'cheap': 1, 'to': 2, 'therefore': 1, 'dangerous': 1}, 'harder': {'than': 1, 'ever.': 1, 'Just': 1}, 'ever.': {'Just': 1, 'last': 1, 'December,': 1}, 'Just': {'last': 1, 'December,': 1, 'fellow': 1}, 'December,': {'fellow': 1, 'economists': 1, 'Martin': 1, 'many': 1, 'gold': 1, 'bugs': 1}, 'fellow': {'economists': 1, 'Martin': 1, 'Feldstein': 1}, 'economists': {'Martin': 1, 'Feldstein': 1, 'and': 1}, 'Martin': {'Feldstein': 1, 'and': 1, 'Nouriel': 1}, 'Feldstein': {'and': 1, 'Nouriel': 1, 'Roubini': 1}, 'and': {'Nouriel': 1, 'Roubini': 1, 'each': 1, 'a': 2, 'rudderless': 1, 'fiscal': 1, 'more': 1, 'direct': 1, 'inflation': 1, 'I': 1, 'discuss': 1, 'in': 2, 'speculate': 1, 'gold.': 1, 'also': 1, 'certain': 1, 'the': 1, 'Middle': 1, 'East': 1, 'to': 1, 'what': 1, 'extent': 1, 'even': 1, 'costs': 1, 'something': 1, 'losses': 1, 'being': 1, 'roughly': 1, 'reason': 1, 'are': 1, 'often': 1}, 'Nouriel': {'Roubini': 1, 'each': 1, 'penned': 1}, 'Roubini': {'each': 1, 'penned': 1, 'op-eds': 1}, 'each': {'penned': 1, 'op-eds': 1, 'bravely': 1}, 'penned': {'op-eds': 1, 'bravely': 1, 'questioning': 1}, 'op-eds': {'bravely': 1, 'questioning': 1, 'bullish': 1}, 'bravely': {'questioning': 1, 'bullish': 1, 'market': 1}, 'questioning': {'bullish': 1, 'market': 1, 'sentiment,': 1}, 'bullish': {'market': 1, 'sentiment,': 1, 'sensibly': 1}, 'market': {'sentiment,': 1, 'sensibly': 1, 'pointing': 1}, 'sentiment,': {'sensibly': 1, 'pointing': 1, 'out': 1}, 'sensibly': {'pointing': 1, 'out': 1, 'gold’s': 1}, 'pointing': {'out': 1, 'gold’s': 1, 'risks.': 1}, 'out': {'gold’s': 1, 'risks.': 1, '<P>': 1}, 'gold’s': {'risks.': 1, '<P>': 1, 'Wouldn’t': 1, 'long': 1, 'upward': 1, 'march': 1, 'appeal': 1, 'has': 1, 'naturally': 1, 'heightened': 1, 'allure': 1, 'in': 1}, 'risks.': {'<P>': 1, 'Wouldn’t': 1, 'you': 1}, 'Wouldn’t': {'you': 1, 'know': 1, 'it?': 1, 'it': 1, 'be': 1, 'paradoxical,': 1}, 'you': {'know': 1, 'it?': 1, 'Since': 1, 'are': 2, 'really': 1, 'worried': 1, 'a': 1, 'high-net-worth': 1}, 'know': {'it?': 1, 'Since': 1, 'their': 1}, 'it?': {'Since': 1, 'their': 1, 'articles': 1}, 'Since': {'their': 1, 'articles': 1, 'appeared,': 1, 'then,': 1, 'the': 1, 'index': 1}, 'their': {'articles': 1, 'appeared,': 1, 'the': 1}, 'articles': {'appeared,': 1, 'the': 1, 'price': 1}, 'appeared,': {'the': 1, 'price': 1, 'of': 1}, 'price': {'of': 3, 'gold': 3, 'has': 1, 'was': 1, 'inevitably': 1, 'headed': 1, 'for': 1, 'is': 4, 'nowhere': 1, 'near': 1, 'probably': 1, 'more': 1, 'scarce': 1, 'commodities.': 1}, 'moved': {'up': 1, 'still': 1, 'further.': 1}, 'still': {'further.': 1, 'Gold': 1, 'prices': 1, 'hold': 1, 'in': 1, 'far': 1}, 'further.': {'Gold': 1, 'prices': 1, 'even': 1}, 'Gold': {'prices': 2, 'even': 1, 'hit': 1, 'are': 1, 'extremely': 1}, 'even': {'hit': 1, 'a': 1, 'record-high': 1, 'higher': 1, 'than': 1, 'that.': 1, 'costs': 1, 'something': 1, 'to': 1}, 'hit': {'a': 1, 'record-high': 1, '$1,300': 1, '$850,': 1, 'or': 1, 'well': 1}, 'record-high': {'$1,300': 1, 'recently.': 1, 'Last': 1}, '$1,300': {'recently.': 1, 'Last': 1, 'December,': 1}, 'recently.': {'Last': 1, 'December,': 1, 'many': 1}, 'Last': {'December,': 1, 'many': 1, 'gold': 1}, 'many': {'gold': 1, 'bugs': 1, 'were': 1, 'countries,': 1, 'it': 1, 'is': 1}, 'bugs': {'were': 1, 'arguing': 1, 'that': 1, 'are': 1, 'right': 1, 'to': 1}, 'were': {'arguing': 1, 'that': 1, 'the': 1}, 'arguing': {'that': 2, 'the': 1, 'price': 1, 'higher': 1, 'gold': 1}, 'that': {'the': 1, 'price': 1, 'was': 1, 'gold': 4, 'could': 1, 'be': 1, 'stock': 1, 'prices': 3, 'languished': 1, 'has': 2, 'crossed': 1, 'it': 2, 'seems.': 1, 'After': 1, 'inflation-indexed': 1, 'bonds': 1, 'offer': 1, 'its': 1, 'value': 1, 'might': 1, 'very': 1, 'high': 1, 'inflation': 1, 'higher': 1, 'gold’s': 1, 'long': 1, 'upward': 1, 'make': 1, 'easier': 1, 'arguably': 1, 'support': 1, 'today’s': 1, 'been': 1, 'sustaining': 1, 'are': 1}, 'was': {'inevitably': 1, 'headed': 1, 'for': 2, 'arguably': 1, 'a': 1, '“freak': 1, 'true': 1, 'the': 1}, 'inevitably': {'headed': 1, 'for': 1, '$2,000.': 1, 'rises,': 1, 'driving': 1, 'up': 1}, 'headed': {'for': 1, '$2,000.': 1, 'Now,': 1, 'even': 1, 'higher': 1, 'than': 1}, 'for': {'$2,000.': 1, 'Now,': 1, 'emboldened': 1, 'a': 1, 'more': 1, 'than': 1, 'gold': 1, 'is': 1, 'not': 1, 'inflation,': 1, 'today’s': 1, 'price': 1, 'ways': 1, 'to': 1, 'transform': 1, 'most': 1, 'of': 2, 'us.': 1, 'the': 1, 'alchemists': 1}, '$2,000.': {'Now,': 1, 'emboldened': 1, 'by': 1}, 'Now,': {'emboldened': 1, 'by': 1, 'continuing': 1}, 'emboldened': {'by': 1, 'continuing': 1, 'appreciation,': 1}, 'by': {'continuing': 1, 'appreciation,': 1, 'some': 1, 'inflation': 1, 'expectations.': 1, 'Some': 1, 'the': 1, 'development': 1, 'of': 1}, 'continuing': {'appreciation,': 1, 'some': 1, 'are': 1}, 'appreciation,': {'some': 1, 'are': 1, 'suggesting': 1}, 'some': {'are': 1, 'suggesting': 1, 'that': 2, 'might': 1, 'argue': 1, 'slight': 1, 'truth': 1, '–': 1}, 'are': {'suggesting': 1, 'that': 2, 'gold': 1, 'really': 1, 'worried': 1, 'about': 1, 'right': 1, 'to': 4, 'worry': 1, 'being': 1, 'driven': 1, 'by': 1, 'solid': 1, 'fundamentals': 1, 'extremely': 1, 'sensitive': 1, 'very': 1, 'difficult': 2, 'a': 1, 'high-net-worth': 1, 'investor,': 1, 'often': 1}, 'suggesting': {'that': 1, 'gold': 1, 'could': 1}, 'could': {'be': 1, 'headed': 1, 'even': 1, 'justify': 1, 'another': 1, 'huge': 1, 'make': 1, 'an': 1, 'ingot': 1, 'plummet.': 1, '<P>': 1, 'Most': 1}, 'be': {'headed': 1, 'even': 1, 'higher': 1, 'the': 1, 'most': 1, 'reliable': 1, 'inflated': 1, 'away.': 1, 'Even': 1, 'cautious': 1, 'in': 1, 'arguing': 1, 'paradoxical,': 1, 'then,': 1, 'if': 1}, 'higher': {'than': 1, 'that.': 1, '<P>': 1, 'price': 1, 'for': 1, 'gold': 3, 'prices': 2, 'are': 1, 'price,': 1, 'although': 1, 'in': 1, 'the': 1}, 'that.': {'<P>': 1, 'One': 1, 'successful': 1}, 'One': {'successful': 1, 'gold': 1, 'investor': 1, 'answer,': 1, 'of': 1, 'course,': 1}, 'successful': {'gold': 1, 'investor': 1, 'recently': 1}, 'investor': {'recently': 1, 'explained': 1, 'to': 1}, 'recently': {'explained': 1, 'to': 1, 'me': 1}, 'explained': {'to': 1, 'me': 1, 'that': 1}, 'me': {'that': 1, 'stock': 1, 'prices': 1}, 'stock': {'prices': 1, 'languished': 1, 'for': 1}, 'languished': {'for': 1, 'a': 1, 'more': 1}, 'decade': {'before': 1, 'the': 1, 'Dow': 1}, 'before': {'the': 1, 'Dow': 1, 'Jones': 1}, 'Dow': {'Jones': 1, 'index': 1, 'crossed': 1}, 'Jones': {'index': 1, 'crossed': 1, 'the': 1}, 'index': {'crossed': 1, 'the': 1, '1,000': 1, 'has': 1, 'climbed': 1, 'above': 1}, 'crossed': {'the': 2, '1,000': 1, 'mark': 1, 'magic': 1, '$1,000': 1}, '1,000': {'mark': 1, 'in': 1, 'the': 1}, 'mark': {'in': 1, 'the': 1, 'early': 1}, 'in': {'the': 3, 'early': 1, '1980’s.': 1, 'today’s': 1, 'dollars.': 1, 'But': 2, 'gold': 3, 'prices': 1, 'from': 1, 'our': 1, 'recent': 1, 'book': 1, 'bond': 1, 'contracts': 1, 'during': 1, 'arguing': 1, 'that': 1, 'higher': 1, 'gold.': 1, '<P>': 2, 'There': 1, 'what': 1, 'we': 1, 'now': 1, 'far': 1, 'lower': 1, 'proportion': 1, 'future.': 1, 'many': 1, 'countries,': 1, 'it': 2, 'instead': 1, 'of': 2, 'bonds.': 1, 'if': 1, 'balance.': 1, 'It': 1, 'is': 1, 'as': 1, 'a': 1, 'wake': 1, 'its': 1, 'price,': 1}, 'early': {'1980’s.': 1, 'Since': 1, 'then,': 1}, '1980’s.': {'Since': 1, 'then,': 1, 'the': 1}, 'then,': {'the': 1, 'index': 1, 'has': 1, 'gold': 1, 'hit': 1, '$850,': 1, 'if': 1, 'financial': 1, 'alchemy': 1}, 'climbed': {'above': 1, '10,000.': 1, 'Now': 1}, 'above': {'10,000.': 1, 'Now': 1, 'that': 1}, '10,000.': {'Now': 1, 'that': 1, 'gold': 1}, 'Now': {'that': 1, 'gold': 1, 'has': 1}, 'magic': {'$1,000': 1, 'barrier,': 1, 'why': 1}, '$1,000': {'barrier,': 1, 'why': 1, 'can’t': 1}, 'barrier,': {'why': 1, 'can’t': 1, 'it': 1}, 'why': {'can’t': 1, 'it': 1, 'increase': 1}, 'can’t': {'it': 1, 'increase': 1, 'ten-fold,': 1}, 'increase': {'ten-fold,': 1, 'too?': 1, '<P>': 1, 'in': 1, 'gold': 1, 'prices': 1}, 'ten-fold,': {'too?': 1, '<P>': 1, 'Admittedly,': 1}, 'too?': {'<P>': 1, 'Admittedly,': 1, 'getting': 1}, 'Admittedly,': {'getting': 1, 'to': 1, 'a': 1}, 'getting': {'to': 1, 'a': 1, 'much': 1}, 'much': {'higher': 1, 'price': 1, 'for': 1}, 'not': {'quite': 1, 'the': 1, 'leap': 1, 'make': 1, 'it': 1, 'probable,': 1}, 'quite': {'the': 1, 'leap': 1, 'of': 1}, 'leap': {'of': 1, 'imagination': 1, 'that': 1}, 'imagination': {'that': 1, 'it': 1, 'seems.': 1}, 'seems.': {'After': 1, 'adjusting': 1, 'for': 1}, 'After': {'adjusting': 1, 'for': 1, 'inflation,': 1, 'all,': 2, 'medieval': 1, 'alchemists': 1, 'gold': 1, 'pays': 1}, 'adjusting': {'for': 1, 'inflation,': 1, 'today’s': 1}, 'inflation,': {'today’s': 1, 'price': 1, 'is': 1}, 'today’s': {'price': 3, 'is': 2, 'nowhere': 1, 'dollars.': 1, 'But': 1, 'January': 1, 'probably': 1, 'high': 1, 'of': 1, 'higher': 1, 'gold': 1, 'price,': 1}, 'nowhere': {'near': 1, 'the': 1, 'all-time': 1}, 'near': {'the': 1, 'all-time': 1, 'high': 1, 'or': 1, 'at': 1, 'record': 1}, 'all-time': {'high': 1, 'of': 1, 'January': 1}, 'high': {'of': 2, 'January': 1, '1980.': 1, 'inflation': 1, 'is': 1, 'possible': 1, 'price': 1, 'gold': 2, 'prices': 1, 'might': 1}, 'January': {'1980.': 1, 'Back': 1, 'then,': 1, '1980': 1, 'was': 1, 'arguably': 1}, '1980.': {'Back': 1, 'then,': 1, 'gold': 1}, 'Back': {'then,': 1, 'gold': 1, 'hit': 1}, '$850,': {'or': 1, 'well': 1, 'over': 1}, 'or': {'well': 1, 'over': 1, '$2,000': 1, 'at': 1, 'record': 1, 'lows': 1, 'a': 1, 'central': 1, 'bank,': 1}, 'well': {'over': 1, '$2,000': 1, 'in': 1, 'they': 1, 'might': 1, 'someday,': 1}, '$2,000': {'in': 1, 'today’s': 1, 'dollars.': 1}, 'dollars.': {'But': 1, 'January': 1, '1980': 1}, 'But': {'January': 1, '1980': 1, 'was': 1, 'gold': 1, 'bugs': 1, 'are': 1, 'if': 1, 'real': 1, 'interest': 1}, '1980': {'was': 1, 'arguably': 1, 'a': 1}, 'arguably': {'a': 1, '“freak': 1, 'peak”': 1, 'support': 1, 'today’s': 1, 'higher': 1}, '“freak': {'peak”': 1, 'during': 1, 'a': 1}, 'peak”': {'during': 1, 'a': 1, 'period': 1}, 'during': {'a': 1, 'period': 1, 'of': 1, 'the': 1, 'Great': 1, 'Depression': 1}, 'period': {'of': 1, 'heightened': 1, 'geo-political': 1}, 'heightened': {'geo-political': 1, 'instability.': 1, 'At': 1, 'allure': 1, 'in': 1, 'the': 1}, 'geo-political': {'instability.': 1, 'At': 1, '$1,300,': 1}, 'instability.': {'At': 1, '$1,300,': 1, 'today’s': 1}, 'At': {'$1,300,': 1, 'today’s': 1, 'price': 1, 'the': 1, 'same': 1, 'time,': 1}, '$1,300,': {'today’s': 1, 'price': 1, 'is': 1}, 'probably': {'more': 1, 'than': 1, 'double': 1, 'some': 1, 'slight': 1, 'truth': 1}, 'double': {'very': 1, 'long-term,': 1, 'inflation-adjusted,': 1}, 'very': {'long-term,': 1, 'inflation-adjusted,': 1, 'average': 1, 'high': 1, 'inflation': 1, 'is': 1, 'difficult': 1, 'to': 1, 'predict': 1, 'risky': 1, 'bet': 1, 'for': 1}, 'long-term,': {'inflation-adjusted,': 1, 'average': 1, 'gold': 1}, 'inflation-adjusted,': {'average': 1, 'gold': 1, 'prices.': 1}, 'average': {'gold': 1, 'prices.': 1, 'So': 1}, 'prices.': {'So': 1, 'what': 1, 'could': 1, 'What': 1, 'was': 1, 'true': 1}, 'So': {'what': 1, 'could': 1, 'justify': 1, 'it': 1, 'can': 1, 'happen': 1}, 'what': {'could': 1, 'justify': 1, 'another': 1, 'we': 1, 'now': 1, 'consider': 1, 'extent': 1, 'they': 1, 'will': 1}, 'justify': {'another': 1, 'huge': 1, 'increase': 1, 'today’s': 1, 'high': 1, 'price': 1}, 'another': {'huge': 1, 'increase': 1, 'in': 1, 'critical': 1, 'fundamental': 1, 'factor': 1}, 'huge': {'increase': 1, 'in': 1, 'gold': 1}, 'from': {'here?': 1, '<P>': 1, 'One': 1, 'the': 1, 'dollar,': 1, 'gold’s': 1, 'short-term': 1, 'trends.': 1, 'Yes,': 1}, 'here?': {'<P>': 1, 'One': 1, 'answer,': 1}, 'answer,': {'of': 1, 'course,': 1, 'is': 1}, 'course,': {'is': 1, 'a': 1, 'complete': 1, 'such': 1, 'considerations': 1, 'might': 1}, 'complete': {'collapse': 1, 'of': 1, 'the': 1}, 'collapse': {'of': 1, 'the': 1, 'US': 1}, 'US': {'dollar.': 1, 'With': 1, 'soaring': 1}, 'dollar.': {'With': 1, 'soaring': 1, 'deficits,': 1}, 'With': {'soaring': 1, 'deficits,': 1, 'and': 1, 'the': 1, 'euro': 1, 'looking': 1}, 'soaring': {'deficits,': 1, 'and': 1, 'a': 1}, 'deficits,': {'and': 1, 'a': 1, 'rudderless': 1}, 'rudderless': {'fiscal': 1, 'policy,': 1, 'one': 1}, 'fiscal': {'policy,': 1, 'one': 1, 'does': 1}, 'policy,': {'one': 1, 'does': 1, 'wonder': 1}, 'one': {'does': 1, 'wonder': 1, 'whether': 1, 'should': 1, 'be': 1, 'cautious': 1}, 'does': {'wonder': 1, 'whether': 1, 'a': 1, 'not': 1, 'make': 1, 'it': 1}, 'wonder': {'whether': 1, 'a': 1, 'populist': 1}, 'whether': {'a': 1, 'populist': 1, 'administration': 1, 'the': 1, 'government': 1, 'will': 1, 'and': 1, 'to': 1, 'what': 1}, 'populist': {'administration': 1, 'might': 1, 'recklessly': 1}, 'administration': {'might': 1, 'recklessly': 1, 'turn': 1}, 'might': {'recklessly': 1, 'turn': 1, 'to': 1, 'indeed': 1, 'be': 2, 'the': 1, 'argue': 1, 'that': 1, 'inflation-indexed': 1, 'inflated': 1, 'away.': 1, 'prove': 1, 'far': 1, 'more': 1, 'someday,': 1, 'gold': 1, 'prices': 1, 'have': 1, 'little': 1, 'influence': 1}, 'recklessly': {'turn': 1, 'to': 1, 'the': 1}, 'turn': {'to': 1, 'the': 1, 'printing': 1}, 'printing': {'press.': 1, 'And': 1, 'if': 1}, 'press.': {'And': 1, 'if': 1, 'you': 1}, 'And': {'if': 1, 'you': 1, 'are': 1}, 'if': {'you': 1, 'are': 1, 'really': 1, 'financial': 1, 'alchemy': 1, 'could': 1, 'real': 1, 'interest': 1, 'rates': 1}, 'really': {'worried': 1, 'about': 1, 'that,': 1}, 'worried': {'about': 1, 'that,': 1, 'gold': 1}, 'that,': {'gold': 1, 'might': 1, 'indeed': 1}, 'indeed': {'be': 1, 'the': 1, 'most': 1}, 'most': {'reliable': 1, 'hedge.': 1, '<P>': 2, 'powerful': 1, 'argument': 1, 'to': 1, 'of': 1, 'us.': 1}, 'reliable': {'hedge.': 1, '<P>': 1, 'Sure,': 1}, 'hedge.': {'<P>': 1, 'Sure,': 1, 'some': 1}, 'Sure,': {'some': 1, 'might': 1, 'argue': 1}, 'argue': {'that': 1, 'inflation-indexed': 1, 'bonds': 1}, 'inflation-indexed': {'bonds': 1, 'offer': 1, 'a': 1}, 'bonds': {'offer': 1, 'a': 1, 'better': 1}, 'offer': {'a': 1, 'better': 1, 'and': 1}, 'better': {'and': 1, 'more': 1, 'direct': 1}, 'direct': {'inflation': 1, 'hedge': 1, 'than': 1}, 'inflation': {'hedge': 1, 'than': 1, 'gold.': 1, 'is': 1, 'possible': 1, 'does': 1, 'expectations.': 1, 'Some': 1, 'have': 1}, 'hedge': {'than': 1, 'gold.': 1, 'But': 1, 'against': 1, 'extreme': 1, 'events.': 1}, 'right': {'to': 1, 'worry': 1, 'about': 1}, 'worry': {'about': 1, 'whether': 1, 'the': 1}, 'government': {'will': 1, 'honor': 1, 'its': 1}, 'will': {'honor': 1, 'its': 1, 'commitments': 1, 'often': 1, 'forcibly': 1, 'convert': 1, 'continue': 1, 'to': 1, 'support': 1}, 'honor': {'its': 1, 'commitments': 1, 'under': 1}, 'its': {'commitments': 1, 'under': 1, 'more': 1, 'value': 1, 'might': 1, 'be': 1, 'price,': 1, 'it': 1, 'remains': 1}, 'commitments': {'under': 1, 'more': 1, 'extreme': 1}, 'under': {'more': 1, 'extreme': 1, 'circumstances.': 1}, 'extreme': {'circumstances.': 1, 'In': 1, 'fact,': 1, 'events.': 1, 'But,': 1, 'despite': 1}, 'circumstances.': {'In': 1, 'fact,': 1, 'as': 1}, 'In': {'fact,': 1, 'as': 1, 'Carmen': 1, 'my': 1, 'view,': 1, 'the': 1}, 'fact,': {'as': 1, 'Carmen': 1, 'Reinhart': 1}, 'as': {'Carmen': 1, 'Reinhart': 1, 'and': 1, 'a': 2, 'diversification': 1, 'play': 1, 'well': 1, 'they': 1, 'might': 1, 'hedge': 1, 'against': 1}, 'Carmen': {'Reinhart': 1, 'and': 1, 'I': 1}, 'Reinhart': {'and': 1, 'I': 1, 'discuss': 1}, 'I': {'discuss': 1, 'in': 1, 'our': 1}, 'discuss': {'in': 1, 'our': 1, 'recent': 1}, 'our': {'recent': 1, 'book': 1, 'on': 1}, 'recent': {'book': 1, 'on': 1, 'the': 1}, 'book': {'on': 1, 'the': 1, 'history': 1}, 'on': {'the': 1, 'history': 1, 'of': 1, 'prices.': 1, 'What': 1, 'was': 1}, 'history': {'of': 1, 'financial': 1, 'crises,': 1}, 'financial': {'crises,': 1, 'This': 1, 'Time': 1, 'instruments': 1, 'that': 1, 'make': 2, 'alchemy': 1, 'could': 1}, 'crises,': {'This': 1, 'Time': 1, 'is': 1}, 'This': {'Time': 1, 'is': 1, 'Different,': 1}, 'Time': {'is': 1, 'Different,': 1, 'cash-strapped': 1}, 'Different,': {'cash-strapped': 1, 'governments': 1, 'will': 1}, 'cash-strapped': {'governments': 1, 'will': 1, 'often': 1}, 'governments': {'will': 1, 'often': 1, 'forcibly': 1}, 'often': {'forcibly': 1, 'convert': 1, 'indexed': 1, 'difficult': 1, 'to': 1, 'reconcile.': 1}, 'forcibly': {'convert': 1, 'indexed': 1, 'debt': 1}, 'convert': {'indexed': 1, 'debt': 1, 'to': 1}, 'indexed': {'debt': 1, 'to': 1, 'non-indexed': 1}, 'debt': {'to': 1, 'non-indexed': 1, 'debt,': 1}, 'non-indexed': {'debt,': 1, 'precisely': 1, 'so': 1}, 'debt,': {'precisely': 1, 'so': 1, 'that': 1}, 'precisely': {'so': 1, 'that': 1, 'its': 1}, 'so': {'that': 1, 'its': 1, 'value': 1, 'one': 1, 'should': 1, 'be': 1}, 'inflated': {'away.': 1, 'Even': 1, 'the': 1}, 'away.': {'Even': 1, 'the': 1, 'United': 1}, 'Even': {'the': 2, 'United': 1, 'States': 1, 'so,': 1, 'fact': 1}, 'United': {'States': 1, 'abrogated': 1, 'indexation': 1}, 'States': {'abrogated': 1, 'indexation': 1, 'clauses': 1}, 'abrogated': {'indexation': 1, 'clauses': 1, 'in': 1}, 'indexation': {'clauses': 1, 'in': 1, 'bond': 1}, 'clauses': {'in': 1, 'bond': 1, 'contracts': 1}, 'bond': {'contracts': 1, 'during': 1, 'the': 1}, 'contracts': {'during': 1, 'the': 1, 'Great': 1}, 'Great': {'Depression': 1, 'of': 1, 'the': 1}, 'Depression': {'of': 1, 'the': 1, '1930’s.': 1}, '1930’s.': {'So': 1, 'it': 1, 'can': 1}, 'can': {'happen': 1, 'anywhere.': 1, '<P>': 1}, 'happen': {'anywhere.': 1, '<P>': 1, 'Even': 1}, 'anywhere.': {'<P>': 1, 'Even': 1, 'so,': 1}, 'so,': {'the': 1, 'fact': 1, 'that': 1, 'too,': 1, 'did': 1, 'worldwide': 1}, 'fact': {'that': 1, 'very': 1, 'high': 1}, 'possible': {'does': 1, 'not': 1, 'make': 1}, 'make': {'it': 2, 'probable,': 1, 'so': 1, 'easier': 1, 'to': 1, 'an': 1, 'ingot': 1, 'of': 1}, 'probable,': {'so': 1, 'one': 1, 'should': 1}, 'should': {'be': 1, 'cautious': 1, 'in': 1}, 'cautious': {'in': 1, 'arguing': 1, 'that': 1}, 'being': {'driven': 1, 'by': 1, 'inflation': 1, 'roughly': 1, 'in': 1, 'balance.': 1}, 'driven': {'by': 2, 'inflation': 1, 'expectations.': 1, 'the': 1, 'development': 1}, 'expectations.': {'Some': 1, 'have': 1, 'argued': 1}, 'Some': {'have': 1, 'argued': 1, 'instead': 1}, 'argued': {'instead': 1, 'that': 1, 'gold’s': 1}, 'instead': {'that': 1, 'gold’s': 1, 'long': 1, 'of': 1, 'investing': 1, 'in': 1}, 'long': {'upward': 1, 'march': 1, 'has': 1}, 'upward': {'march': 1, 'has': 1, 'been': 1}, 'march': {'has': 1, 'been': 1, 'partly': 1}, 'partly': {'driven': 1, 'by': 1, 'the': 1}, 'development': {'of': 1, 'new': 1, 'financial': 1}, 'new': {'financial': 1, 'instruments': 1, 'that': 1, 'consumers': 1, 'gain': 1, 'purchasing': 1}, 'instruments': {'that': 1, 'make': 1, 'it': 1}, 'easier': {'to': 1, 'trade': 1, 'and': 1}, 'trade': {'and': 1, 'speculate': 1, 'in': 1}, 'speculate': {'in': 2, 'gold.': 1, '<P>': 1, 'gold': 1, 'instead': 1}, 'There': {'is': 1, 'probably': 1, 'some': 1}, 'slight': {'truth': 1, '–': 1, 'and': 1}, 'truth': {'–': 1, 'and': 1, 'also': 1}, 'also': {'a': 1, 'certain': 1, 'degree': 1}, 'certain': {'degree': 1, 'of': 1, 'irony': 1}, 'degree': {'of': 1, 'irony': 1, '–': 1}, 'irony': {'–': 1, 'to': 1, 'this': 1}, 'this': {'argument.': 1, 'After': 1, 'all,': 1}, 'argument.': {'After': 1, 'all,': 1, 'medieval': 1}, 'all,': {'medieval': 1, 'alchemists': 1, 'engaged': 1, 'gold': 1, 'pays': 1, 'no': 1}, 'medieval': {'alchemists': 1, 'engaged': 1, 'in': 1}, 'alchemists': {'engaged': 1, 'in': 1, 'what': 1, 'of': 1, 'yore': 1, 'remains': 1}, 'engaged': {'in': 1, 'what': 1, 'we': 1}, 'we': {'now': 1, 'consider': 1, 'an': 1}, 'now': {'consider': 1, 'an': 1, 'absurd': 1}, 'consider': {'an': 1, 'absurd': 1, 'search': 1}, 'an': {'absurd': 1, 'search': 1, 'for': 1, 'ingot': 1, 'of': 1, 'gold': 1, 'extraordinary': 1, 'run-up': 1, 'in': 1}, 'absurd': {'search': 1, 'for': 1, 'ways': 1}, 'search': {'for': 1, 'ways': 1, 'to': 1}, 'ways': {'to': 1, 'transform': 1, 'base': 1}, 'transform': {'base': 1, 'metals': 1, 'into': 1}, 'base': {'metals': 1, 'into': 1, 'gold.': 1}, 'metals': {'into': 1, 'gold.': 1, 'Wouldn’t': 1}, 'into': {'gold.': 1, 'Wouldn’t': 1, 'it': 1, 'the': 1, 'global': 1, 'economy.': 1}, 'paradoxical,': {'then,': 1, 'if': 1, 'financial': 1}, 'alchemy': {'could': 1, 'make': 1, 'an': 1}, 'ingot': {'of': 1, 'gold': 1, 'worth': 1}, 'worth': {'dramatically': 1, 'more?': 1, '<P>': 1}, 'dramatically': {'more?': 1, '<P>': 1, 'In': 1}, 'more?': {'<P>': 1, 'In': 1, 'my': 1}, 'my': {'view,': 1, 'the': 1, 'most': 1}, 'view,': {'the': 1, 'most': 1, 'powerful': 1}, 'powerful': {'argument': 1, 'to': 1, 'justify': 1}, 'argument': {'to': 1, 'justify': 1, 'today’s': 1}, 'dramatic': {'emergence': 1, 'of': 1, 'Asia,': 1}, 'emergence': {'of': 1, 'Asia,': 1, 'Latin': 1}, 'Asia,': {'Latin': 1, 'America,': 1, 'and': 1}, 'Latin': {'America,': 1, 'and': 1, 'the': 1}, 'America,': {'and': 1, 'the': 1, 'Middle': 1}, 'Middle': {'East': 1, 'into': 1, 'the': 1}, 'East': {'into': 1, 'the': 1, 'global': 1}, 'global': {'economy.': 1, 'As': 1, 'legions': 1, 'interest-rate': 1, 'movements.': 1, 'After': 1}, 'economy.': {'As': 1, 'legions': 1, 'of': 1}, 'As': {'legions': 1, 'of': 1, 'new': 1}, 'legions': {'of': 1, 'new': 1, 'consumers': 1}, 'consumers': {'gain': 1, 'purchasing': 1, 'power,': 1}, 'gain': {'purchasing': 1, 'power,': 1, 'demand': 1}, 'purchasing': {'power,': 1, 'demand': 1, 'inevitably': 1}, 'power,': {'demand': 1, 'inevitably': 1, 'rises,': 1}, 'demand': {'inevitably': 1, 'rises,': 1, 'driving': 1}, 'rises,': {'driving': 1, 'up': 1, 'the': 1}, 'driving': {'up': 1, 'the': 1, 'price': 1}, 'scarce': {'commodities.': 1, '<P>': 1, 'At': 1}, 'commodities.': {'<P>': 1, 'At': 1, 'the': 1}, 'same': {'time,': 1, 'emerging-market': 1, 'central': 1}, 'time,': {'emerging-market': 1, 'central': 1, 'banks': 1}, 'emerging-market': {'central': 1, 'banks': 1, 'need': 1}, 'central': {'banks': 1, 'need': 1, 'to': 1, 'banks.': 1, 'With': 1, 'the': 1, 'bank,': 1, 'it': 1, 'makes': 1}, 'banks': {'need': 1, 'to': 1, 'accumulate': 1}, 'need': {'to': 1, 'accumulate': 1, 'gold': 1}, 'accumulate': {'gold': 1, 'reserves,': 1, 'which': 1}, 'reserves,': {'which': 1, 'they': 1, 'still': 1}, 'which': {'they': 1, 'still': 1, 'hold': 1}, 'they': {'still': 1, 'hold': 1, 'in': 1, 'will': 1, 'continue': 1, 'to': 1, 'might': 1, 'someday,': 1, 'gold': 1}, 'hold': {'in': 1, 'far': 1, 'lower': 1, 'a': 1, 'modest': 1, 'proportion': 1}, 'far': {'lower': 1, 'proportion': 1, 'than': 2, 'more': 2, 'debatable': 1, 'whether': 1, 'ephemeral': 1}, 'lower': {'proportion': 1, 'than': 1, 'do': 1}, 'proportion': {'than': 1, 'do': 1, 'rich-country': 1, 'of': 1, 'your': 1, 'portfolio': 1}, 'do': {'rich-country': 1, 'central': 1, 'banks.': 1}, 'rich-country': {'central': 1, 'banks.': 1, 'With': 1}, 'banks.': {'With': 1, 'the': 1, 'euro': 1}, 'euro': {'looking': 1, 'less': 1, 'appetizing': 1}, 'looking': {'less': 1, 'appetizing': 1, 'as': 1}, 'less': {'appetizing': 1, 'as': 1, 'a': 1}, 'appetizing': {'as': 1, 'a': 1, 'diversification': 1}, 'diversification': {'play': 1, 'away': 1, 'from': 1}, 'play': {'away': 1, 'from': 1, 'the': 1}, 'away': {'from': 1, 'the': 1, 'dollar,': 1}, 'dollar,': {'gold’s': 1, 'appeal': 1, 'has': 1}, 'appeal': {'has': 1, 'naturally': 1, 'grown.': 1}, 'naturally': {'grown.': 1, '<P>': 1, 'So,': 1}, 'grown.': {'<P>': 1, 'So,': 1, 'yes,': 1}, 'So,': {'yes,': 1, 'there': 1, 'are': 1}, 'yes,': {'there': 1, 'are': 1, 'solid': 1}, 'there': {'are': 1, 'solid': 1, 'fundamentals': 1}, 'solid': {'fundamentals': 1, 'that': 1, 'arguably': 1}, 'fundamentals': {'that': 1, 'arguably': 1, 'support': 1}, 'support': {'today’s': 1, 'higher': 2, 'gold': 1, 'prices': 1, 'in': 1}, 'price,': {'although': 1, 'it': 2, 'is': 1, 'remains': 1, 'a': 1}, 'although': {'it': 1, 'is': 1, 'far': 1}, 'debatable': {'whether': 1, 'and': 1, 'to': 1}, 'extent': {'they': 1, 'will': 1, 'continue': 1}, 'continue': {'to': 1, 'support': 1, 'higher': 1}, 'future.': {'<P>': 1, 'Indeed,': 1, 'another': 1}, 'Indeed,': {'another': 1, 'critical': 1, 'fundamental': 1}, 'critical': {'fundamental': 1, 'factor': 1, 'that': 1}, 'fundamental': {'factor': 1, 'that': 1, 'has': 1}, 'factor': {'that': 1, 'has': 1, 'been': 1}, 'sustaining': {'high': 1, 'gold': 1, 'prices': 1}, 'prove': {'far': 1, 'more': 1, 'ephemeral': 1}, 'ephemeral': {'than': 1, 'globalization.': 1, 'Gold': 1}, 'globalization.': {'Gold': 1, 'prices': 1, 'are': 1}, 'extremely': {'sensitive': 1, 'to': 1, 'global': 1}, 'sensitive': {'to': 1, 'global': 1, 'interest-rate': 1}, 'interest-rate': {'movements.': 1, 'After': 1, 'all,': 1}, 'movements.': {'After': 1, 'all,': 1, 'gold': 1}, 'pays': {'no': 1, 'interest': 1, 'and': 1}, 'no': {'interest': 1, 'and': 1, 'even': 1}, 'interest': {'and': 1, 'even': 1, 'costs': 1, 'rates': 2, 'near': 1, 'or': 1, 'rise': 1, 'significantly,': 1}, 'costs': {'something': 1, 'to': 1, 'store.': 1}, 'something': {'to': 1, 'store.': 1, 'Today,': 1}, 'store.': {'Today,': 1, 'with': 1, 'interest': 1}, 'Today,': {'with': 1, 'interest': 1, 'rates': 1}, 'rates': {'near': 1, 'or': 1, 'at': 1, 'rise': 1, 'significantly,': 1, 'as': 1}, 'at': {'record': 1, 'lows': 1, 'in': 1}, 'record': {'lows': 1, 'in': 1, 'many': 1}, 'lows': {'in': 1, 'many': 1, 'countries,': 1}, 'countries,': {'it': 1, 'is': 1, 'relatively': 1}, 'relatively': {'cheap': 1, 'to': 1, 'speculate': 1}, 'cheap': {'to': 1, 'speculate': 1, 'in': 1}, 'investing': {'in': 1, 'bonds.': 1, 'But': 1}, 'bonds.': {'But': 1, 'if': 1, 'real': 1}, 'real': {'interest': 1, 'rates': 1, 'rise': 1}, 'rise': {'significantly,': 1, 'as': 1, 'well': 1}, 'significantly,': {'as': 1, 'well': 1, 'they': 1}, 'someday,': {'gold': 1, 'prices': 1, 'could': 1}, 'plummet.': {'<P>': 1, 'Most': 1, 'economic': 1}, 'Most': {'economic': 1, 'research': 1, 'suggests': 1}, 'economic': {'research': 1, 'suggests': 1, 'that': 1}, 'research': {'suggests': 1, 'that': 1, 'gold': 1}, 'suggests': {'that': 1, 'gold': 1, 'prices': 1}, 'difficult': {'to': 2, 'predict': 1, 'over': 1, 'reconcile.': 1}, 'predict': {'over': 1, 'the': 1, 'short': 1}, 'short': {'to': 1, 'medium': 1, 'term,': 1}, 'medium': {'term,': 1, 'with': 1, 'the': 1}, 'term,': {'with': 1, 'the': 1, 'odds': 1}, 'odds': {'of': 1, 'gains': 1, 'and': 1}, 'gains': {'and': 1, 'losses': 1, 'being': 1}, 'losses': {'being': 1, 'roughly': 1, 'in': 1}, 'roughly': {'in': 1, 'balance.': 1, 'It': 1}, 'balance.': {'It': 1, 'is': 1, 'therefore': 1}, 'therefore': {'dangerous': 1, 'to': 1, 'extrapolate': 1}, 'dangerous': {'to': 1, 'extrapolate': 1, 'from': 1}, 'extrapolate': {'from': 1, 'short-term': 1, 'trends.': 1}, 'short-term': {'trends.': 1, 'Yes,': 1, 'gold': 1}, 'trends.': {'Yes,': 1, 'gold': 1, 'has': 1}, 'Yes,': {'gold': 1, 'has': 1, 'had': 1}, 'had': {'a': 1, 'great': 1, 'run,': 1}, 'great': {'run,': 1, 'but': 1, 'so,': 1}, 'run,': {'but': 1, 'so,': 1, 'too,': 1}, 'but': {'so,': 1, 'too,': 1, 'did': 1}, 'too,': {'did': 1, 'worldwide': 1, 'housing': 1}, 'did': {'worldwide': 1, 'housing': 1, 'prices': 1}, 'worldwide': {'housing': 1, 'prices': 1, 'until': 1}, 'housing': {'prices': 1, 'until': 1, 'a': 1}, 'until': {'a': 1, 'couple': 1, 'of': 1}, 'couple': {'of': 1, 'years': 1, 'ago.': 1}, 'years': {'ago.': 1, '<P>': 1, 'If': 1}, 'ago.': {'<P>': 1, 'If': 1, 'you': 1}, 'If': {'you': 1, 'are': 1, 'a': 1}, 'high-net-worth': {'investor,': 1, 'a': 1, 'sovereign': 1}, 'investor,': {'a': 1, 'sovereign': 1, 'wealth': 1}, 'sovereign': {'wealth': 1, 'fund,': 1, 'or': 1}, 'wealth': {'fund,': 1, 'or': 1, 'a': 1}, 'fund,': {'or': 1, 'a': 1, 'central': 1}, 'bank,': {'it': 1, 'makes': 1, 'perfect': 1}, 'makes': {'perfect': 1, 'sense': 1, 'to': 1}, 'perfect': {'sense': 1, 'to': 1, 'hold': 1}, 'sense': {'to': 1, 'hold': 1, 'a': 1}, 'modest': {'proportion': 1, 'of': 1, 'your': 1}, 'your': {'portfolio': 1, 'in': 1, 'gold': 1}, 'portfolio': {'in': 1, 'gold': 1, 'as': 1}, 'against': {'extreme': 1, 'events.': 1, 'But,': 1}, 'events.': {'But,': 1, 'despite': 1, 'gold’s': 1}, 'But,': {'despite': 1, 'gold’s': 1, 'heightened': 1}, 'despite': {'gold’s': 1, 'heightened': 1, 'allure': 1}, 'allure': {'in': 1, 'the': 1, 'wake': 1}, 'wake': {'of': 1, 'an': 1, 'extraordinary': 1}, 'extraordinary': {'run-up': 1, 'in': 1, 'its': 1}, 'run-up': {'in': 1, 'its': 1, 'price,': 1}, 'remains': {'a': 1, 'very': 1, 'risky': 1, 'true': 1, 'today:': 1, 'gold': 1}, 'risky': {'bet': 1, 'for': 1, 'most': 1}, 'bet': {'for': 1, 'most': 1, 'of': 1}, 'us.': {'<P>': 1, 'Of': 1, 'course,': 1}, 'Of': {'course,': 1, 'such': 1, 'considerations': 1}, 'such': {'considerations': 1, 'might': 1, 'have': 1}, 'considerations': {'might': 1, 'have': 1, 'little': 1}, 'little': {'influence': 1, 'on': 1, 'prices.': 1}, 'influence': {'on': 1, 'prices.': 1, 'What': 1}, 'What': {'was': 1, 'true': 1, 'for': 1}, 'true': {'for': 1, 'the': 1, 'alchemists': 1, 'today:': 1, 'gold': 1, 'and': 1}, 'yore': {'remains': 1, 'true': 1, 'today:': 1}, 'today:': {'gold': 1, 'and': 1, 'reason': 1}, 'reason': {'are': 1, 'often': 1, 'difficult': 1}, 'reconcile.': {}}\n"
     ]
    }
   ],
   "source": [
    "#Mini = 1 rapport succint avec comment éxecuter le script etc...\n",
    "#pouvoir guider le prof sur la correction\n",
    "#On envoie que le code et il regenere tout\n",
    "\n",
    "def create_cooc_mat(filePath,window):\n",
    "    file = open(filePath,'r')\n",
    "    matrice = {}\n",
    "    with open(filePath)as file:\n",
    "        data = file.read()\n",
    "        i = 0\n",
    "        for word in data.split():\n",
    "            if word not in matrice:\n",
    "                matrice[word] = {}\n",
    "            for y in range(0,window):\n",
    "                if i+y+1 < len(data.split()) and data.split()[i+y+1] not in matrice[word]:\n",
    "                    matrice[word][data.split()[i+y+1]] = 1\n",
    "                elif i+y+1 < len(data.split()) :\n",
    "                    matrice[word][data.split()[i+y+1]] += 1\n",
    "            i +=1\n",
    "    return matrice\n",
    "                \n",
    "cooc_mat = create_cooc_mat('./English/-10-000-gold.txt',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 11:30:48,002 : INFO : Generating the array of data in order to train the model with those data..\n",
      "2019-11-24 11:30:51,697 : INFO : Array generated ! \n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "#sentences = MySentences('/home/sebastien/Documents/Polytech/NLP/English/') # a memory-friendly iterator\n",
    "def getDataForModel(PathToRead):\n",
    "    logging.info(\"Generating the array of data in order to train the model with those data..\")\n",
    "    data = []\n",
    "    onlyfiles = [f for f in listdir(PathToRead) if isfile(join(PathToRead, f))]\n",
    "    for file in onlyfiles:\n",
    "        with open(PathToRead+file, encoding ='utf8') as f:\n",
    "            data.append(f.read().replace('\\n', ' '))\n",
    "    sentences = []\n",
    "    sentencesCut = []\n",
    "    for doc in data:\n",
    "        sentences.append(doc.split(' . '))\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            sTrim = re.sub(r',', ' ', s)\n",
    "            sTrim = re.sub(r' ’ s', '', sTrim)\n",
    "            sentencesCut.append(sTrim.split())\n",
    "    logging.info(\"Array generated ! \")\n",
    "    return (sentencesCut,sentences)\n",
    "\n",
    "dataForModel,dataForFasttext = getDataForModel('D:/Documents/Polytech/NLP/leme/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'December', 'fellow', 'economist', 'Martin', 'Feldstein', 'Nouriel', 'Roubini', 'penned', 'op-eds', 'bravely', 'questioning', 'bullish', 'market', 'sentiment', 'sensibly', 'pointing', 'gold', 'risk']\n"
     ]
    }
   ],
   "source": [
    "print(dataForModel[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:56:41,244 : INFO : Start to generate all the model, 2 skip gram (100/300) and 2 CBOW (100/300)\n",
      "2019-11-23 21:56:41,328 : INFO : Generating skip-gram 300...\n",
      "2019-11-23 21:56:41,328 : INFO : collecting all words and their counts\n",
      "2019-11-23 21:56:41,329 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-23 21:56:41,355 : INFO : PROGRESS: at sentence #10000, processed 167000 words, keeping 17523 word types\n",
      "2019-11-23 21:56:41,382 : INFO : PROGRESS: at sentence #20000, processed 337041 words, keeping 26301 word types\n",
      "2019-11-23 21:56:41,412 : INFO : PROGRESS: at sentence #30000, processed 509923 words, keeping 32059 word types\n",
      "2019-11-23 21:56:41,440 : INFO : PROGRESS: at sentence #40000, processed 675666 words, keeping 37429 word types\n",
      "2019-11-23 21:56:41,470 : INFO : PROGRESS: at sentence #50000, processed 848467 words, keeping 42432 word types\n",
      "2019-11-23 21:56:41,503 : INFO : PROGRESS: at sentence #60000, processed 1027013 words, keeping 46793 word types\n",
      "2019-11-23 21:56:41,535 : INFO : PROGRESS: at sentence #70000, processed 1206037 words, keeping 51639 word types\n",
      "2019-11-23 21:56:41,563 : INFO : PROGRESS: at sentence #80000, processed 1374828 words, keeping 55379 word types\n",
      "2019-11-23 21:56:41,594 : INFO : PROGRESS: at sentence #90000, processed 1549470 words, keeping 59059 word types\n",
      "2019-11-23 21:56:41,627 : INFO : PROGRESS: at sentence #100000, processed 1712414 words, keeping 61695 word types\n",
      "2019-11-23 21:56:41,653 : INFO : PROGRESS: at sentence #110000, processed 1870537 words, keeping 63879 word types\n",
      "2019-11-23 21:56:41,682 : INFO : PROGRESS: at sentence #120000, processed 2038071 words, keeping 66693 word types\n",
      "2019-11-23 21:56:41,712 : INFO : PROGRESS: at sentence #130000, processed 2210836 words, keeping 69489 word types\n",
      "2019-11-23 21:56:41,739 : INFO : PROGRESS: at sentence #140000, processed 2378732 words, keeping 72048 word types\n",
      "2019-11-23 21:56:41,768 : INFO : PROGRESS: at sentence #150000, processed 2547595 words, keeping 74682 word types\n",
      "2019-11-23 21:56:41,798 : INFO : PROGRESS: at sentence #160000, processed 2710933 words, keeping 76857 word types\n",
      "2019-11-23 21:56:41,827 : INFO : PROGRESS: at sentence #170000, processed 2887205 words, keeping 79128 word types\n",
      "2019-11-23 21:56:41,856 : INFO : PROGRESS: at sentence #180000, processed 3056359 words, keeping 81292 word types\n",
      "2019-11-23 21:56:41,884 : INFO : PROGRESS: at sentence #190000, processed 3217513 words, keeping 83246 word types\n",
      "2019-11-23 21:56:41,915 : INFO : PROGRESS: at sentence #200000, processed 3393418 words, keeping 85724 word types\n",
      "2019-11-23 21:56:41,952 : INFO : PROGRESS: at sentence #210000, processed 3564657 words, keeping 87764 word types\n",
      "2019-11-23 21:56:41,984 : INFO : PROGRESS: at sentence #220000, processed 3737994 words, keeping 90124 word types\n",
      "2019-11-23 21:56:42,016 : INFO : PROGRESS: at sentence #230000, processed 3909410 words, keeping 92315 word types\n",
      "2019-11-23 21:56:42,045 : INFO : PROGRESS: at sentence #240000, processed 4076085 words, keeping 94700 word types\n",
      "2019-11-23 21:56:42,076 : INFO : PROGRESS: at sentence #250000, processed 4246615 words, keeping 97318 word types\n",
      "2019-11-23 21:56:42,110 : INFO : PROGRESS: at sentence #260000, processed 4422306 words, keeping 99447 word types\n",
      "2019-11-23 21:56:42,141 : INFO : PROGRESS: at sentence #270000, processed 4585696 words, keeping 101136 word types\n",
      "2019-11-23 21:56:42,170 : INFO : PROGRESS: at sentence #280000, processed 4746200 words, keeping 102626 word types\n",
      "2019-11-23 21:56:42,197 : INFO : PROGRESS: at sentence #290000, processed 4906782 words, keeping 103938 word types\n",
      "2019-11-23 21:56:42,225 : INFO : PROGRESS: at sentence #300000, processed 5072215 words, keeping 105508 word types\n",
      "2019-11-23 21:56:42,256 : INFO : PROGRESS: at sentence #310000, processed 5234047 words, keeping 106897 word types\n",
      "2019-11-23 21:56:42,287 : INFO : PROGRESS: at sentence #320000, processed 5394363 words, keeping 108360 word types\n",
      "2019-11-23 21:56:42,318 : INFO : PROGRESS: at sentence #330000, processed 5556622 words, keeping 109844 word types\n",
      "2019-11-23 21:56:42,348 : INFO : PROGRESS: at sentence #340000, processed 5717834 words, keeping 111098 word types\n",
      "2019-11-23 21:56:42,382 : INFO : PROGRESS: at sentence #350000, processed 5882201 words, keeping 112562 word types\n",
      "2019-11-23 21:56:42,414 : INFO : PROGRESS: at sentence #360000, processed 6048246 words, keeping 114000 word types\n",
      "2019-11-23 21:56:42,445 : INFO : PROGRESS: at sentence #370000, processed 6213062 words, keeping 115180 word types\n",
      "2019-11-23 21:56:42,462 : INFO : collected 115867 word types from a corpus of 6297818 raw words and 374816 sentences\n",
      "2019-11-23 21:56:42,463 : INFO : Loading a fresh vocabulary\n",
      "2019-11-23 21:56:42,620 : INFO : effective_min_count=1 retains 115867 unique words (100% of original 115867, drops 0)\n",
      "2019-11-23 21:56:42,620 : INFO : effective_min_count=1 leaves 6297818 word corpus (100% of original 6297818, drops 0)\n",
      "2019-11-23 21:56:42,916 : INFO : deleting the raw counts dictionary of 115867 items\n",
      "2019-11-23 21:56:42,919 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2019-11-23 21:56:42,920 : INFO : downsampling leaves estimated 5246232 word corpus (83.3% of prior 6297818)\n",
      "2019-11-23 21:56:43,182 : INFO : estimated required memory for 115867 words and 300 dimensions: 336014300 bytes\n",
      "2019-11-23 21:56:43,182 : INFO : resetting layer weights\n",
      "2019-11-23 21:56:44,574 : INFO : training model with 3 workers on 115867 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-23 21:56:45,603 : INFO : EPOCH 1 - PROGRESS: at 4.73% examples, 240241 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:46,610 : INFO : EPOCH 1 - PROGRESS: at 9.73% examples, 251456 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:47,613 : INFO : EPOCH 1 - PROGRESS: at 14.42% examples, 250528 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:48,632 : INFO : EPOCH 1 - PROGRESS: at 19.16% examples, 253408 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:49,639 : INFO : EPOCH 1 - PROGRESS: at 24.45% examples, 258962 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:50,667 : INFO : EPOCH 1 - PROGRESS: at 30.04% examples, 261638 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-23 21:56:51,686 : INFO : EPOCH 1 - PROGRESS: at 35.57% examples, 265244 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:52,702 : INFO : EPOCH 1 - PROGRESS: at 41.06% examples, 267907 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:53,724 : INFO : EPOCH 1 - PROGRESS: at 46.39% examples, 268789 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:54,736 : INFO : EPOCH 1 - PROGRESS: at 51.99% examples, 270724 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:55,765 : INFO : EPOCH 1 - PROGRESS: at 57.04% examples, 270342 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:56,787 : INFO : EPOCH 1 - PROGRESS: at 62.03% examples, 269630 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:57,806 : INFO : EPOCH 1 - PROGRESS: at 67.24% examples, 269635 words/s, in_qsize 4, out_qsize 1\n",
      "2019-11-23 21:56:58,853 : INFO : EPOCH 1 - PROGRESS: at 72.44% examples, 269160 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:56:59,877 : INFO : EPOCH 1 - PROGRESS: at 77.92% examples, 269098 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:00,886 : INFO : EPOCH 1 - PROGRESS: at 83.45% examples, 269744 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:01,932 : INFO : EPOCH 1 - PROGRESS: at 89.23% examples, 270239 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:02,960 : INFO : EPOCH 1 - PROGRESS: at 94.24% examples, 269147 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:03,979 : INFO : EPOCH 1 - PROGRESS: at 99.43% examples, 268718 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:04,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:57:04,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:57:04,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:57:04,090 : INFO : EPOCH - 1 : training on 6297818 raw words (5246111 effective words) took 19.5s, 268881 effective words/s\n",
      "2019-11-23 21:57:05,128 : INFO : EPOCH 2 - PROGRESS: at 5.36% examples, 270261 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:57:06,157 : INFO : EPOCH 2 - PROGRESS: at 11.00% examples, 280373 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:07,205 : INFO : EPOCH 2 - PROGRESS: at 16.46% examples, 281709 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:08,264 : INFO : EPOCH 2 - PROGRESS: at 21.52% examples, 276457 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:09,275 : INFO : EPOCH 2 - PROGRESS: at 26.73% examples, 275406 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:10,321 : INFO : EPOCH 2 - PROGRESS: at 32.55% examples, 277308 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:11,364 : INFO : EPOCH 2 - PROGRESS: at 38.29% examples, 278860 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:12,411 : INFO : EPOCH 2 - PROGRESS: at 43.94% examples, 279661 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:13,453 : INFO : EPOCH 2 - PROGRESS: at 49.67% examples, 280401 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:14,519 : INFO : EPOCH 2 - PROGRESS: at 55.22% examples, 280592 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:15,578 : INFO : EPOCH 2 - PROGRESS: at 60.73% examples, 280855 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:16,614 : INFO : EPOCH 2 - PROGRESS: at 66.47% examples, 281567 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:17,657 : INFO : EPOCH 2 - PROGRESS: at 72.10% examples, 282065 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:18,694 : INFO : EPOCH 2 - PROGRESS: at 78.08% examples, 282556 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:19,727 : INFO : EPOCH 2 - PROGRESS: at 83.96% examples, 283010 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:20,764 : INFO : EPOCH 2 - PROGRESS: at 89.91% examples, 283343 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:21,800 : INFO : EPOCH 2 - PROGRESS: at 95.72% examples, 283647 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:22,499 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:57:22,503 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:57:22,565 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:57:22,566 : INFO : EPOCH - 2 : training on 6297818 raw words (5246609 effective words) took 18.5s, 284046 effective words/s\n",
      "2019-11-23 21:57:23,579 : INFO : EPOCH 3 - PROGRESS: at 5.36% examples, 277059 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:24,615 : INFO : EPOCH 3 - PROGRESS: at 11.00% examples, 282955 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:25,639 : INFO : EPOCH 3 - PROGRESS: at 16.46% examples, 285463 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:26,667 : INFO : EPOCH 3 - PROGRESS: at 21.85% examples, 285358 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:27,707 : INFO : EPOCH 3 - PROGRESS: at 27.59% examples, 285804 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:28,736 : INFO : EPOCH 3 - PROGRESS: at 33.21% examples, 285447 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:29,750 : INFO : EPOCH 3 - PROGRESS: at 38.45% examples, 283524 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:30,757 : INFO : EPOCH 3 - PROGRESS: at 43.48% examples, 281050 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:31,794 : INFO : EPOCH 3 - PROGRESS: at 48.65% examples, 279138 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:32,795 : INFO : EPOCH 3 - PROGRESS: at 53.34% examples, 276340 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:33,839 : INFO : EPOCH 3 - PROGRESS: at 58.44% examples, 275016 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:34,856 : INFO : EPOCH 3 - PROGRESS: at 63.59% examples, 274689 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:35,858 : INFO : EPOCH 3 - PROGRESS: at 68.91% examples, 275310 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:36,863 : INFO : EPOCH 3 - PROGRESS: at 74.25% examples, 275186 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:37,912 : INFO : EPOCH 3 - PROGRESS: at 79.52% examples, 273730 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:38,940 : INFO : EPOCH 3 - PROGRESS: at 84.96% examples, 273272 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:39,983 : INFO : EPOCH 3 - PROGRESS: at 90.55% examples, 273129 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:40,983 : INFO : EPOCH 3 - PROGRESS: at 95.55% examples, 272288 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:41,831 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:57:41,840 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:57:41,903 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:57:41,904 : INFO : EPOCH - 3 : training on 6297818 raw words (5246045 effective words) took 19.3s, 271350 effective words/s\n",
      "2019-11-23 21:57:42,923 : INFO : EPOCH 4 - PROGRESS: at 4.58% examples, 234365 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:43,946 : INFO : EPOCH 4 - PROGRESS: at 9.57% examples, 246714 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:44,961 : INFO : EPOCH 4 - PROGRESS: at 14.37% examples, 249041 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:45,974 : INFO : EPOCH 4 - PROGRESS: at 18.86% examples, 248419 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:46,977 : INFO : EPOCH 4 - PROGRESS: at 24.11% examples, 255266 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:47,984 : INFO : EPOCH 4 - PROGRESS: at 29.89% examples, 260816 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-23 21:57:49,003 : INFO : EPOCH 4 - PROGRESS: at 34.42% examples, 257553 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:50,041 : INFO : EPOCH 4 - PROGRESS: at 40.13% examples, 261558 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:51,060 : INFO : EPOCH 4 - PROGRESS: at 44.84% examples, 259570 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:52,116 : INFO : EPOCH 4 - PROGRESS: at 50.11% examples, 259586 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:53,134 : INFO : EPOCH 4 - PROGRESS: at 55.06% examples, 259882 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:54,134 : INFO : EPOCH 4 - PROGRESS: at 60.47% examples, 262445 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:55,159 : INFO : EPOCH 4 - PROGRESS: at 65.87% examples, 263558 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:56,185 : INFO : EPOCH 4 - PROGRESS: at 71.28% examples, 265092 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:57,197 : INFO : EPOCH 4 - PROGRESS: at 76.73% examples, 265483 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:58,223 : INFO : EPOCH 4 - PROGRESS: at 82.14% examples, 265577 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:57:59,250 : INFO : EPOCH 4 - PROGRESS: at 87.58% examples, 265661 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:00,259 : INFO : EPOCH 4 - PROGRESS: at 93.32% examples, 266897 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:01,267 : INFO : EPOCH 4 - PROGRESS: at 98.63% examples, 267176 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:01,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:58:01,515 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:58:01,540 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:58:01,541 : INFO : EPOCH - 4 : training on 6297818 raw words (5246791 effective words) took 19.6s, 267251 effective words/s\n",
      "2019-11-23 21:58:02,579 : INFO : EPOCH 5 - PROGRESS: at 5.36% examples, 270349 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:03,603 : INFO : EPOCH 5 - PROGRESS: at 10.72% examples, 273296 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:04,639 : INFO : EPOCH 5 - PROGRESS: at 16.01% examples, 275144 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:05,645 : INFO : EPOCH 5 - PROGRESS: at 21.09% examples, 274814 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:06,676 : INFO : EPOCH 5 - PROGRESS: at 26.73% examples, 278025 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:07,696 : INFO : EPOCH 5 - PROGRESS: at 32.55% examples, 280662 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:08,743 : INFO : EPOCH 5 - PROGRESS: at 38.11% examples, 280509 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:09,747 : INFO : EPOCH 5 - PROGRESS: at 43.17% examples, 278540 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:10,756 : INFO : EPOCH 5 - PROGRESS: at 47.97% examples, 275899 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:58:11,770 : INFO : EPOCH 5 - PROGRESS: at 53.51% examples, 277166 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:12,795 : INFO : EPOCH 5 - PROGRESS: at 58.75% examples, 277020 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:13,801 : INFO : EPOCH 5 - PROGRESS: at 64.26% examples, 278123 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:14,804 : INFO : EPOCH 5 - PROGRESS: at 69.04% examples, 276560 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:15,841 : INFO : EPOCH 5 - PROGRESS: at 74.42% examples, 275739 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:16,864 : INFO : EPOCH 5 - PROGRESS: at 80.34% examples, 276889 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:17,868 : INFO : EPOCH 5 - PROGRESS: at 85.96% examples, 277130 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:18,882 : INFO : EPOCH 5 - PROGRESS: at 91.73% examples, 277697 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:19,897 : INFO : EPOCH 5 - PROGRESS: at 97.48% examples, 278635 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:20,279 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:58:20,333 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:58:20,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:58:20,350 : INFO : EPOCH - 5 : training on 6297818 raw words (5246289 effective words) took 18.8s, 278999 effective words/s\n",
      "2019-11-23 21:58:20,350 : INFO : training on a 31489090 raw words (26231845 effective words) took 95.8s, 273890 effective words/s\n",
      "2019-11-23 21:58:20,393 : INFO : saving Word2Vec object under ./model/word2vec300SG, separately None\n",
      "2019-11-23 21:58:20,394 : INFO : storing np array 'vectors' to ./model/word2vec300SG.wv.vectors.npy\n",
      "2019-11-23 21:58:20,990 : INFO : not storing attribute vectors_norm\n",
      "2019-11-23 21:58:20,993 : INFO : storing np array 'syn1neg' to ./model/word2vec300SG.trainables.syn1neg.npy\n",
      "2019-11-23 21:58:21,636 : INFO : not storing attribute cum_table\n",
      "2019-11-23 21:58:21,830 : INFO : saved ./model/word2vec300SG\n",
      "2019-11-23 21:58:21,830 : INFO : Generating skip-gram 100...\n",
      "2019-11-23 21:58:21,831 : INFO : collecting all words and their counts\n",
      "2019-11-23 21:58:21,831 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-23 21:58:21,855 : INFO : PROGRESS: at sentence #10000, processed 167000 words, keeping 17523 word types\n",
      "2019-11-23 21:58:21,880 : INFO : PROGRESS: at sentence #20000, processed 337041 words, keeping 26301 word types\n",
      "2019-11-23 21:58:21,906 : INFO : PROGRESS: at sentence #30000, processed 509923 words, keeping 32059 word types\n",
      "2019-11-23 21:58:21,934 : INFO : PROGRESS: at sentence #40000, processed 675666 words, keeping 37429 word types\n",
      "2019-11-23 21:58:21,964 : INFO : PROGRESS: at sentence #50000, processed 848467 words, keeping 42432 word types\n",
      "2019-11-23 21:58:21,995 : INFO : PROGRESS: at sentence #60000, processed 1027013 words, keeping 46793 word types\n",
      "2019-11-23 21:58:22,025 : INFO : PROGRESS: at sentence #70000, processed 1206037 words, keeping 51639 word types\n",
      "2019-11-23 21:58:22,053 : INFO : PROGRESS: at sentence #80000, processed 1374828 words, keeping 55379 word types\n",
      "2019-11-23 21:58:22,081 : INFO : PROGRESS: at sentence #90000, processed 1549470 words, keeping 59059 word types\n",
      "2019-11-23 21:58:22,107 : INFO : PROGRESS: at sentence #100000, processed 1712414 words, keeping 61695 word types\n",
      "2019-11-23 21:58:22,132 : INFO : PROGRESS: at sentence #110000, processed 1870537 words, keeping 63879 word types\n",
      "2019-11-23 21:58:22,159 : INFO : PROGRESS: at sentence #120000, processed 2038071 words, keeping 66693 word types\n",
      "2019-11-23 21:58:22,188 : INFO : PROGRESS: at sentence #130000, processed 2210836 words, keeping 69489 word types\n",
      "2019-11-23 21:58:22,214 : INFO : PROGRESS: at sentence #140000, processed 2378732 words, keeping 72048 word types\n",
      "2019-11-23 21:58:22,242 : INFO : PROGRESS: at sentence #150000, processed 2547595 words, keeping 74682 word types\n",
      "2019-11-23 21:58:22,268 : INFO : PROGRESS: at sentence #160000, processed 2710933 words, keeping 76857 word types\n",
      "2019-11-23 21:58:22,297 : INFO : PROGRESS: at sentence #170000, processed 2887205 words, keeping 79128 word types\n",
      "2019-11-23 21:58:22,323 : INFO : PROGRESS: at sentence #180000, processed 3056359 words, keeping 81292 word types\n",
      "2019-11-23 21:58:22,349 : INFO : PROGRESS: at sentence #190000, processed 3217513 words, keeping 83246 word types\n",
      "2019-11-23 21:58:22,378 : INFO : PROGRESS: at sentence #200000, processed 3393418 words, keeping 85724 word types\n",
      "2019-11-23 21:58:22,410 : INFO : PROGRESS: at sentence #210000, processed 3564657 words, keeping 87764 word types\n",
      "2019-11-23 21:58:22,439 : INFO : PROGRESS: at sentence #220000, processed 3737994 words, keeping 90124 word types\n",
      "2019-11-23 21:58:22,468 : INFO : PROGRESS: at sentence #230000, processed 3909410 words, keeping 92315 word types\n",
      "2019-11-23 21:58:22,497 : INFO : PROGRESS: at sentence #240000, processed 4076085 words, keeping 94700 word types\n",
      "2019-11-23 21:58:22,525 : INFO : PROGRESS: at sentence #250000, processed 4246615 words, keeping 97318 word types\n",
      "2019-11-23 21:58:22,554 : INFO : PROGRESS: at sentence #260000, processed 4422306 words, keeping 99447 word types\n",
      "2019-11-23 21:58:22,581 : INFO : PROGRESS: at sentence #270000, processed 4585696 words, keeping 101136 word types\n",
      "2019-11-23 21:58:22,607 : INFO : PROGRESS: at sentence #280000, processed 4746200 words, keeping 102626 word types\n",
      "2019-11-23 21:58:22,634 : INFO : PROGRESS: at sentence #290000, processed 4906782 words, keeping 103938 word types\n",
      "2019-11-23 21:58:22,664 : INFO : PROGRESS: at sentence #300000, processed 5072215 words, keeping 105508 word types\n",
      "2019-11-23 21:58:22,690 : INFO : PROGRESS: at sentence #310000, processed 5234047 words, keeping 106897 word types\n",
      "2019-11-23 21:58:22,717 : INFO : PROGRESS: at sentence #320000, processed 5394363 words, keeping 108360 word types\n",
      "2019-11-23 21:58:22,744 : INFO : PROGRESS: at sentence #330000, processed 5556622 words, keeping 109844 word types\n",
      "2019-11-23 21:58:22,771 : INFO : PROGRESS: at sentence #340000, processed 5717834 words, keeping 111098 word types\n",
      "2019-11-23 21:58:22,798 : INFO : PROGRESS: at sentence #350000, processed 5882201 words, keeping 112562 word types\n",
      "2019-11-23 21:58:22,825 : INFO : PROGRESS: at sentence #360000, processed 6048246 words, keeping 114000 word types\n",
      "2019-11-23 21:58:22,852 : INFO : PROGRESS: at sentence #370000, processed 6213062 words, keeping 115180 word types\n",
      "2019-11-23 21:58:22,867 : INFO : collected 115867 word types from a corpus of 6297818 raw words and 374816 sentences\n",
      "2019-11-23 21:58:22,867 : INFO : Loading a fresh vocabulary\n",
      "2019-11-23 21:58:23,236 : INFO : effective_min_count=1 retains 115867 unique words (100% of original 115867, drops 0)\n",
      "2019-11-23 21:58:23,236 : INFO : effective_min_count=1 leaves 6297818 word corpus (100% of original 6297818, drops 0)\n",
      "2019-11-23 21:58:23,526 : INFO : deleting the raw counts dictionary of 115867 items\n",
      "2019-11-23 21:58:23,529 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2019-11-23 21:58:23,530 : INFO : downsampling leaves estimated 5246232 word corpus (83.3% of prior 6297818)\n",
      "2019-11-23 21:58:23,782 : INFO : estimated required memory for 115867 words and 100 dimensions: 150627100 bytes\n",
      "2019-11-23 21:58:23,783 : INFO : resetting layer weights\n",
      "2019-11-23 21:58:24,924 : INFO : training model with 3 workers on 115867 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-23 21:58:25,969 : INFO : EPOCH 1 - PROGRESS: at 8.15% examples, 411710 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:26,977 : INFO : EPOCH 1 - PROGRESS: at 16.31% examples, 423780 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:27,978 : INFO : EPOCH 1 - PROGRESS: at 23.06% examples, 405212 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:28,992 : INFO : EPOCH 1 - PROGRESS: at 30.85% examples, 402401 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:29,996 : INFO : EPOCH 1 - PROGRESS: at 39.36% examples, 411577 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:31,004 : INFO : EPOCH 1 - PROGRESS: at 47.97% examples, 418325 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:32,011 : INFO : EPOCH 1 - PROGRESS: at 56.11% examples, 420025 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:58:33,018 : INFO : EPOCH 1 - PROGRESS: at 64.58% examples, 423385 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:34,029 : INFO : EPOCH 1 - PROGRESS: at 73.11% examples, 425909 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:35,035 : INFO : EPOCH 1 - PROGRESS: at 81.97% examples, 427889 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:36,036 : INFO : EPOCH 1 - PROGRESS: at 90.72% examples, 428959 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:37,069 : INFO : EPOCH 1 - PROGRESS: at 99.58% examples, 430074 words/s, in_qsize 4, out_qsize 0\n",
      "2019-11-23 21:58:37,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:58:37,086 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:58:37,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:58:37,121 : INFO : EPOCH - 1 : training on 6297818 raw words (5245932 effective words) took 12.2s, 430300 effective words/s\n",
      "2019-11-23 21:58:38,134 : INFO : EPOCH 2 - PROGRESS: at 8.47% examples, 440770 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:39,135 : INFO : EPOCH 2 - PROGRESS: at 16.75% examples, 443978 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:40,143 : INFO : EPOCH 2 - PROGRESS: at 25.07% examples, 445160 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:41,156 : INFO : EPOCH 2 - PROGRESS: at 33.99% examples, 446914 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:42,161 : INFO : EPOCH 2 - PROGRESS: at 42.57% examples, 447107 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:43,167 : INFO : EPOCH 2 - PROGRESS: at 51.22% examples, 448190 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:44,187 : INFO : EPOCH 2 - PROGRESS: at 59.38% examples, 446025 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:45,221 : INFO : EPOCH 2 - PROGRESS: at 66.76% examples, 437446 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:46,237 : INFO : EPOCH 2 - PROGRESS: at 74.76% examples, 434433 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:47,242 : INFO : EPOCH 2 - PROGRESS: at 82.30% examples, 429121 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:48,248 : INFO : EPOCH 2 - PROGRESS: at 90.24% examples, 426116 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:49,261 : INFO : EPOCH 2 - PROGRESS: at 98.31% examples, 424785 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:49,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:58:49,443 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:58:49,471 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:58:49,472 : INFO : EPOCH - 2 : training on 6297818 raw words (5246354 effective words) took 12.3s, 424917 effective words/s\n",
      "2019-11-23 21:58:50,477 : INFO : EPOCH 3 - PROGRESS: at 7.24% examples, 377855 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:51,479 : INFO : EPOCH 3 - PROGRESS: at 15.10% examples, 400464 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:52,485 : INFO : EPOCH 3 - PROGRESS: at 23.31% examples, 416177 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:53,494 : INFO : EPOCH 3 - PROGRESS: at 31.83% examples, 419249 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:54,495 : INFO : EPOCH 3 - PROGRESS: at 39.51% examples, 417228 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:55,504 : INFO : EPOCH 3 - PROGRESS: at 47.33% examples, 416131 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:56,522 : INFO : EPOCH 3 - PROGRESS: at 54.73% examples, 411678 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:57,538 : INFO : EPOCH 3 - PROGRESS: at 62.65% examples, 412437 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:58,562 : INFO : EPOCH 3 - PROGRESS: at 70.46% examples, 411910 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:58:59,573 : INFO : EPOCH 3 - PROGRESS: at 78.88% examples, 412671 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:00,598 : INFO : EPOCH 3 - PROGRESS: at 87.74% examples, 414967 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:01,599 : INFO : EPOCH 3 - PROGRESS: at 96.20% examples, 416324 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:02,052 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:59:02,083 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:59:02,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:59:02,101 : INFO : EPOCH - 3 : training on 6297818 raw words (5245981 effective words) took 12.6s, 415559 effective words/s\n",
      "2019-11-23 21:59:03,108 : INFO : EPOCH 4 - PROGRESS: at 8.15% examples, 426957 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:04,118 : INFO : EPOCH 4 - PROGRESS: at 16.61% examples, 439191 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:05,142 : INFO : EPOCH 4 - PROGRESS: at 25.07% examples, 442338 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:06,155 : INFO : EPOCH 4 - PROGRESS: at 33.99% examples, 444856 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:07,157 : INFO : EPOCH 4 - PROGRESS: at 42.57% examples, 445658 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:08,158 : INFO : EPOCH 4 - PROGRESS: at 51.22% examples, 447319 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:09,160 : INFO : EPOCH 4 - PROGRESS: at 59.52% examples, 447665 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:10,165 : INFO : EPOCH 4 - PROGRESS: at 68.01% examples, 447635 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:11,186 : INFO : EPOCH 4 - PROGRESS: at 76.91% examples, 447733 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:12,198 : INFO : EPOCH 4 - PROGRESS: at 85.96% examples, 448140 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:13,204 : INFO : EPOCH 4 - PROGRESS: at 94.89% examples, 448735 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:13,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:59:13,786 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:59:13,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:59:13,805 : INFO : EPOCH - 4 : training on 6297818 raw words (5245564 effective words) took 11.7s, 448388 effective words/s\n",
      "2019-11-23 21:59:14,847 : INFO : EPOCH 5 - PROGRESS: at 8.64% examples, 436914 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:15,859 : INFO : EPOCH 5 - PROGRESS: at 17.03% examples, 443652 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:16,861 : INFO : EPOCH 5 - PROGRESS: at 25.41% examples, 445806 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:17,884 : INFO : EPOCH 5 - PROGRESS: at 33.51% examples, 436093 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:18,891 : INFO : EPOCH 5 - PROGRESS: at 42.07% examples, 438276 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:19,903 : INFO : EPOCH 5 - PROGRESS: at 50.61% examples, 438992 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:20,911 : INFO : EPOCH 5 - PROGRESS: at 58.75% examples, 438860 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:21,922 : INFO : EPOCH 5 - PROGRESS: at 67.24% examples, 439774 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:22,935 : INFO : EPOCH 5 - PROGRESS: at 75.90% examples, 440234 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:23,948 : INFO : EPOCH 5 - PROGRESS: at 84.96% examples, 441358 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:24,948 : INFO : EPOCH 5 - PROGRESS: at 93.77% examples, 441990 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:25,642 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:59:25,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:59:25,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:59:25,684 : INFO : EPOCH - 5 : training on 6297818 raw words (5246919 effective words) took 11.9s, 441891 effective words/s\n",
      "2019-11-23 21:59:25,685 : INFO : training on a 31489090 raw words (26230750 effective words) took 60.8s, 431716 effective words/s\n",
      "2019-11-23 21:59:25,685 : INFO : saving Word2Vec object under ./model/word2vec100SG, separately None\n",
      "2019-11-23 21:59:25,686 : INFO : storing np array 'vectors' to ./model/word2vec100SG.wv.vectors.npy\n",
      "2019-11-23 21:59:25,849 : INFO : not storing attribute vectors_norm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:59:25,850 : INFO : storing np array 'syn1neg' to ./model/word2vec100SG.trainables.syn1neg.npy\n",
      "2019-11-23 21:59:25,999 : INFO : not storing attribute cum_table\n",
      "2019-11-23 21:59:26,211 : INFO : saved ./model/word2vec100SG\n",
      "2019-11-23 21:59:26,212 : INFO : Generating CBOW 300...\n",
      "2019-11-23 21:59:26,212 : INFO : collecting all words and their counts\n",
      "2019-11-23 21:59:26,212 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-23 21:59:26,237 : INFO : PROGRESS: at sentence #10000, processed 167000 words, keeping 17523 word types\n",
      "2019-11-23 21:59:26,263 : INFO : PROGRESS: at sentence #20000, processed 337041 words, keeping 26301 word types\n",
      "2019-11-23 21:59:26,290 : INFO : PROGRESS: at sentence #30000, processed 509923 words, keeping 32059 word types\n",
      "2019-11-23 21:59:26,316 : INFO : PROGRESS: at sentence #40000, processed 675666 words, keeping 37429 word types\n",
      "2019-11-23 21:59:26,343 : INFO : PROGRESS: at sentence #50000, processed 848467 words, keeping 42432 word types\n",
      "2019-11-23 21:59:26,374 : INFO : PROGRESS: at sentence #60000, processed 1027013 words, keeping 46793 word types\n",
      "2019-11-23 21:59:26,403 : INFO : PROGRESS: at sentence #70000, processed 1206037 words, keeping 51639 word types\n",
      "2019-11-23 21:59:26,431 : INFO : PROGRESS: at sentence #80000, processed 1374828 words, keeping 55379 word types\n",
      "2019-11-23 21:59:26,461 : INFO : PROGRESS: at sentence #90000, processed 1549470 words, keeping 59059 word types\n",
      "2019-11-23 21:59:26,488 : INFO : PROGRESS: at sentence #100000, processed 1712414 words, keeping 61695 word types\n",
      "2019-11-23 21:59:26,515 : INFO : PROGRESS: at sentence #110000, processed 1870537 words, keeping 63879 word types\n",
      "2019-11-23 21:59:26,543 : INFO : PROGRESS: at sentence #120000, processed 2038071 words, keeping 66693 word types\n",
      "2019-11-23 21:59:26,573 : INFO : PROGRESS: at sentence #130000, processed 2210836 words, keeping 69489 word types\n",
      "2019-11-23 21:59:26,601 : INFO : PROGRESS: at sentence #140000, processed 2378732 words, keeping 72048 word types\n",
      "2019-11-23 21:59:26,630 : INFO : PROGRESS: at sentence #150000, processed 2547595 words, keeping 74682 word types\n",
      "2019-11-23 21:59:26,657 : INFO : PROGRESS: at sentence #160000, processed 2710933 words, keeping 76857 word types\n",
      "2019-11-23 21:59:26,686 : INFO : PROGRESS: at sentence #170000, processed 2887205 words, keeping 79128 word types\n",
      "2019-11-23 21:59:26,718 : INFO : PROGRESS: at sentence #180000, processed 3056359 words, keeping 81292 word types\n",
      "2019-11-23 21:59:26,745 : INFO : PROGRESS: at sentence #190000, processed 3217513 words, keeping 83246 word types\n",
      "2019-11-23 21:59:26,776 : INFO : PROGRESS: at sentence #200000, processed 3393418 words, keeping 85724 word types\n",
      "2019-11-23 21:59:26,807 : INFO : PROGRESS: at sentence #210000, processed 3564657 words, keeping 87764 word types\n",
      "2019-11-23 21:59:26,837 : INFO : PROGRESS: at sentence #220000, processed 3737994 words, keeping 90124 word types\n",
      "2019-11-23 21:59:26,867 : INFO : PROGRESS: at sentence #230000, processed 3909410 words, keeping 92315 word types\n",
      "2019-11-23 21:59:26,895 : INFO : PROGRESS: at sentence #240000, processed 4076085 words, keeping 94700 word types\n",
      "2019-11-23 21:59:26,927 : INFO : PROGRESS: at sentence #250000, processed 4246615 words, keeping 97318 word types\n",
      "2019-11-23 21:59:26,959 : INFO : PROGRESS: at sentence #260000, processed 4422306 words, keeping 99447 word types\n",
      "2019-11-23 21:59:26,987 : INFO : PROGRESS: at sentence #270000, processed 4585696 words, keeping 101136 word types\n",
      "2019-11-23 21:59:27,014 : INFO : PROGRESS: at sentence #280000, processed 4746200 words, keeping 102626 word types\n",
      "2019-11-23 21:59:27,040 : INFO : PROGRESS: at sentence #290000, processed 4906782 words, keeping 103938 word types\n",
      "2019-11-23 21:59:27,070 : INFO : PROGRESS: at sentence #300000, processed 5072215 words, keeping 105508 word types\n",
      "2019-11-23 21:59:27,099 : INFO : PROGRESS: at sentence #310000, processed 5234047 words, keeping 106897 word types\n",
      "2019-11-23 21:59:27,130 : INFO : PROGRESS: at sentence #320000, processed 5394363 words, keeping 108360 word types\n",
      "2019-11-23 21:59:27,159 : INFO : PROGRESS: at sentence #330000, processed 5556622 words, keeping 109844 word types\n",
      "2019-11-23 21:59:27,189 : INFO : PROGRESS: at sentence #340000, processed 5717834 words, keeping 111098 word types\n",
      "2019-11-23 21:59:27,219 : INFO : PROGRESS: at sentence #350000, processed 5882201 words, keeping 112562 word types\n",
      "2019-11-23 21:59:27,250 : INFO : PROGRESS: at sentence #360000, processed 6048246 words, keeping 114000 word types\n",
      "2019-11-23 21:59:27,278 : INFO : PROGRESS: at sentence #370000, processed 6213062 words, keeping 115180 word types\n",
      "2019-11-23 21:59:27,294 : INFO : collected 115867 word types from a corpus of 6297818 raw words and 374816 sentences\n",
      "2019-11-23 21:59:27,295 : INFO : Loading a fresh vocabulary\n",
      "2019-11-23 21:59:27,441 : INFO : effective_min_count=1 retains 115867 unique words (100% of original 115867, drops 0)\n",
      "2019-11-23 21:59:27,442 : INFO : effective_min_count=1 leaves 6297818 word corpus (100% of original 6297818, drops 0)\n",
      "2019-11-23 21:59:27,751 : INFO : deleting the raw counts dictionary of 115867 items\n",
      "2019-11-23 21:59:27,754 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2019-11-23 21:59:27,755 : INFO : downsampling leaves estimated 5246232 word corpus (83.3% of prior 6297818)\n",
      "2019-11-23 21:59:28,036 : INFO : estimated required memory for 115867 words and 300 dimensions: 336014300 bytes\n",
      "2019-11-23 21:59:28,037 : INFO : resetting layer weights\n",
      "2019-11-23 21:59:29,478 : INFO : training model with 3 workers on 115867 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-23 21:59:30,488 : INFO : EPOCH 1 - PROGRESS: at 15.86% examples, 839139 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:31,493 : INFO : EPOCH 1 - PROGRESS: at 33.81% examples, 892578 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:32,497 : INFO : EPOCH 1 - PROGRESS: at 51.37% examples, 901562 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:33,504 : INFO : EPOCH 1 - PROGRESS: at 68.61% examples, 905605 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:34,510 : INFO : EPOCH 1 - PROGRESS: at 87.09% examples, 911496 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:35,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:59:35,203 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:59:35,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:59:35,207 : INFO : EPOCH - 1 : training on 6297818 raw words (5246435 effective words) took 5.7s, 916408 effective words/s\n",
      "2019-11-23 21:59:36,216 : INFO : EPOCH 2 - PROGRESS: at 17.64% examples, 940210 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:37,219 : INFO : EPOCH 2 - PROGRESS: at 35.07% examples, 927443 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:38,221 : INFO : EPOCH 2 - PROGRESS: at 51.22% examples, 900576 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-23 21:59:39,234 : INFO : EPOCH 2 - PROGRESS: at 64.41% examples, 849799 words/s, in_qsize 3, out_qsize 2\n",
      "2019-11-23 21:59:40,245 : INFO : EPOCH 2 - PROGRESS: at 78.08% examples, 819959 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:41,271 : INFO : EPOCH 2 - PROGRESS: at 92.88% examples, 804390 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-23 21:59:41,821 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:59:41,822 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:59:41,843 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:59:41,844 : INFO : EPOCH - 2 : training on 6297818 raw words (5246957 effective words) took 6.6s, 791362 effective words/s\n",
      "2019-11-23 21:59:42,856 : INFO : EPOCH 3 - PROGRESS: at 15.86% examples, 837992 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:43,859 : INFO : EPOCH 3 - PROGRESS: at 32.88% examples, 867704 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:44,869 : INFO : EPOCH 3 - PROGRESS: at 49.95% examples, 874985 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:45,877 : INFO : EPOCH 3 - PROGRESS: at 67.24% examples, 885616 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:59:46,886 : INFO : EPOCH 3 - PROGRESS: at 85.12% examples, 890079 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:47,690 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:59:47,693 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:59:47,704 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:59:47,705 : INFO : EPOCH - 3 : training on 6297818 raw words (5246991 effective words) took 5.9s, 896093 effective words/s\n",
      "2019-11-23 21:59:48,710 : INFO : EPOCH 4 - PROGRESS: at 16.61% examples, 883891 words/s, in_qsize 6, out_qsize 1\n",
      "2019-11-23 21:59:49,716 : INFO : EPOCH 4 - PROGRESS: at 33.51% examples, 884987 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:50,727 : INFO : EPOCH 4 - PROGRESS: at 51.22% examples, 897227 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-23 21:59:51,731 : INFO : EPOCH 4 - PROGRESS: at 68.75% examples, 907519 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:52,736 : INFO : EPOCH 4 - PROGRESS: at 86.76% examples, 908107 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:53,471 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:59:53,472 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:59:53,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:59:53,480 : INFO : EPOCH - 4 : training on 6297818 raw words (5246138 effective words) took 5.8s, 909083 effective words/s\n",
      "2019-11-23 21:59:54,487 : INFO : EPOCH 5 - PROGRESS: at 17.64% examples, 940378 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-23 21:59:55,493 : INFO : EPOCH 5 - PROGRESS: at 35.72% examples, 942786 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:56,495 : INFO : EPOCH 5 - PROGRESS: at 53.51% examples, 941494 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:57,501 : INFO : EPOCH 5 - PROGRESS: at 70.63% examples, 933964 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:58,506 : INFO : EPOCH 5 - PROGRESS: at 89.23% examples, 933823 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 21:59:59,117 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:59:59,119 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:59:59,131 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:59:59,131 : INFO : EPOCH - 5 : training on 6297818 raw words (5245331 effective words) took 5.6s, 928929 effective words/s\n",
      "2019-11-23 21:59:59,132 : INFO : training on a 31489090 raw words (26231852 effective words) took 29.7s, 884596 effective words/s\n",
      "2019-11-23 21:59:59,133 : INFO : saving Word2Vec object under ./model/word2vec300CBOW, separately None\n",
      "2019-11-23 21:59:59,134 : INFO : storing np array 'vectors' to ./model/word2vec300CBOW.wv.vectors.npy\n",
      "2019-11-23 21:59:59,761 : INFO : not storing attribute vectors_norm\n",
      "2019-11-23 21:59:59,762 : INFO : storing np array 'syn1neg' to ./model/word2vec300CBOW.trainables.syn1neg.npy\n",
      "2019-11-23 22:00:00,380 : INFO : not storing attribute cum_table\n",
      "2019-11-23 22:00:00,598 : INFO : saved ./model/word2vec300CBOW\n",
      "2019-11-23 22:00:00,599 : INFO : Generating CBOW 100...\n",
      "2019-11-23 22:00:00,600 : INFO : collecting all words and their counts\n",
      "2019-11-23 22:00:00,600 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-23 22:00:00,622 : INFO : PROGRESS: at sentence #10000, processed 167000 words, keeping 17523 word types\n",
      "2019-11-23 22:00:00,647 : INFO : PROGRESS: at sentence #20000, processed 337041 words, keeping 26301 word types\n",
      "2019-11-23 22:00:00,672 : INFO : PROGRESS: at sentence #30000, processed 509923 words, keeping 32059 word types\n",
      "2019-11-23 22:00:00,696 : INFO : PROGRESS: at sentence #40000, processed 675666 words, keeping 37429 word types\n",
      "2019-11-23 22:00:00,726 : INFO : PROGRESS: at sentence #50000, processed 848467 words, keeping 42432 word types\n",
      "2019-11-23 22:00:00,755 : INFO : PROGRESS: at sentence #60000, processed 1027013 words, keeping 46793 word types\n",
      "2019-11-23 22:00:00,785 : INFO : PROGRESS: at sentence #70000, processed 1206037 words, keeping 51639 word types\n",
      "2019-11-23 22:00:00,810 : INFO : PROGRESS: at sentence #80000, processed 1374828 words, keeping 55379 word types\n",
      "2019-11-23 22:00:00,837 : INFO : PROGRESS: at sentence #90000, processed 1549470 words, keeping 59059 word types\n",
      "2019-11-23 22:00:00,861 : INFO : PROGRESS: at sentence #100000, processed 1712414 words, keeping 61695 word types\n",
      "2019-11-23 22:00:00,889 : INFO : PROGRESS: at sentence #110000, processed 1870537 words, keeping 63879 word types\n",
      "2019-11-23 22:00:00,915 : INFO : PROGRESS: at sentence #120000, processed 2038071 words, keeping 66693 word types\n",
      "2019-11-23 22:00:00,941 : INFO : PROGRESS: at sentence #130000, processed 2210836 words, keeping 69489 word types\n",
      "2019-11-23 22:00:00,967 : INFO : PROGRESS: at sentence #140000, processed 2378732 words, keeping 72048 word types\n",
      "2019-11-23 22:00:00,992 : INFO : PROGRESS: at sentence #150000, processed 2547595 words, keeping 74682 word types\n",
      "2019-11-23 22:00:01,018 : INFO : PROGRESS: at sentence #160000, processed 2710933 words, keeping 76857 word types\n",
      "2019-11-23 22:00:01,047 : INFO : PROGRESS: at sentence #170000, processed 2887205 words, keeping 79128 word types\n",
      "2019-11-23 22:00:01,075 : INFO : PROGRESS: at sentence #180000, processed 3056359 words, keeping 81292 word types\n",
      "2019-11-23 22:00:01,101 : INFO : PROGRESS: at sentence #190000, processed 3217513 words, keeping 83246 word types\n",
      "2019-11-23 22:00:01,130 : INFO : PROGRESS: at sentence #200000, processed 3393418 words, keeping 85724 word types\n",
      "2019-11-23 22:00:01,159 : INFO : PROGRESS: at sentence #210000, processed 3564657 words, keeping 87764 word types\n",
      "2019-11-23 22:00:01,186 : INFO : PROGRESS: at sentence #220000, processed 3737994 words, keeping 90124 word types\n",
      "2019-11-23 22:00:01,214 : INFO : PROGRESS: at sentence #230000, processed 3909410 words, keeping 92315 word types\n",
      "2019-11-23 22:00:01,241 : INFO : PROGRESS: at sentence #240000, processed 4076085 words, keeping 94700 word types\n",
      "2019-11-23 22:00:01,270 : INFO : PROGRESS: at sentence #250000, processed 4246615 words, keeping 97318 word types\n",
      "2019-11-23 22:00:01,299 : INFO : PROGRESS: at sentence #260000, processed 4422306 words, keeping 99447 word types\n",
      "2019-11-23 22:00:01,326 : INFO : PROGRESS: at sentence #270000, processed 4585696 words, keeping 101136 word types\n",
      "2019-11-23 22:00:01,351 : INFO : PROGRESS: at sentence #280000, processed 4746200 words, keeping 102626 word types\n",
      "2019-11-23 22:00:01,375 : INFO : PROGRESS: at sentence #290000, processed 4906782 words, keeping 103938 word types\n",
      "2019-11-23 22:00:01,404 : INFO : PROGRESS: at sentence #300000, processed 5072215 words, keeping 105508 word types\n",
      "2019-11-23 22:00:01,429 : INFO : PROGRESS: at sentence #310000, processed 5234047 words, keeping 106897 word types\n",
      "2019-11-23 22:00:01,455 : INFO : PROGRESS: at sentence #320000, processed 5394363 words, keeping 108360 word types\n",
      "2019-11-23 22:00:01,481 : INFO : PROGRESS: at sentence #330000, processed 5556622 words, keeping 109844 word types\n",
      "2019-11-23 22:00:01,506 : INFO : PROGRESS: at sentence #340000, processed 5717834 words, keeping 111098 word types\n",
      "2019-11-23 22:00:01,532 : INFO : PROGRESS: at sentence #350000, processed 5882201 words, keeping 112562 word types\n",
      "2019-11-23 22:00:01,558 : INFO : PROGRESS: at sentence #360000, processed 6048246 words, keeping 114000 word types\n",
      "2019-11-23 22:00:01,583 : INFO : PROGRESS: at sentence #370000, processed 6213062 words, keeping 115180 word types\n",
      "2019-11-23 22:00:01,599 : INFO : collected 115867 word types from a corpus of 6297818 raw words and 374816 sentences\n",
      "2019-11-23 22:00:01,599 : INFO : Loading a fresh vocabulary\n",
      "2019-11-23 22:00:01,995 : INFO : effective_min_count=1 retains 115867 unique words (100% of original 115867, drops 0)\n",
      "2019-11-23 22:00:01,996 : INFO : effective_min_count=1 leaves 6297818 word corpus (100% of original 6297818, drops 0)\n",
      "2019-11-23 22:00:02,285 : INFO : deleting the raw counts dictionary of 115867 items\n",
      "2019-11-23 22:00:02,288 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2019-11-23 22:00:02,288 : INFO : downsampling leaves estimated 5246232 word corpus (83.3% of prior 6297818)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:00:02,541 : INFO : estimated required memory for 115867 words and 100 dimensions: 150627100 bytes\n",
      "2019-11-23 22:00:02,541 : INFO : resetting layer weights\n",
      "2019-11-23 22:00:03,669 : INFO : training model with 3 workers on 115867 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-23 22:00:04,677 : INFO : EPOCH 1 - PROGRESS: at 24.45% examples, 1308075 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:05,683 : INFO : EPOCH 1 - PROGRESS: at 50.11% examples, 1320037 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:06,685 : INFO : EPOCH 1 - PROGRESS: at 75.58% examples, 1329154 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:07,665 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 22:00:07,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 22:00:07,678 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 22:00:07,679 : INFO : EPOCH - 1 : training on 6297818 raw words (5247001 effective words) took 4.0s, 1310616 effective words/s\n",
      "2019-11-23 22:00:08,684 : INFO : EPOCH 2 - PROGRESS: at 21.42% examples, 1141811 words/s, in_qsize 4, out_qsize 1\n",
      "2019-11-23 22:00:09,687 : INFO : EPOCH 2 - PROGRESS: at 41.08% examples, 1086082 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:10,690 : INFO : EPOCH 2 - PROGRESS: at 61.38% examples, 1083743 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:11,696 : INFO : EPOCH 2 - PROGRESS: at 82.63% examples, 1085858 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-23 22:00:12,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 22:00:12,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 22:00:12,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 22:00:12,505 : INFO : EPOCH - 2 : training on 6297818 raw words (5246268 effective words) took 4.8s, 1088014 effective words/s\n",
      "2019-11-23 22:00:13,521 : INFO : EPOCH 3 - PROGRESS: at 19.16% examples, 1016053 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:14,526 : INFO : EPOCH 3 - PROGRESS: at 36.47% examples, 960172 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:15,527 : INFO : EPOCH 3 - PROGRESS: at 54.57% examples, 958584 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:16,535 : INFO : EPOCH 3 - PROGRESS: at 77.41% examples, 1016616 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:17,413 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 22:00:17,417 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 22:00:17,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 22:00:17,421 : INFO : EPOCH - 3 : training on 6297818 raw words (5246734 effective words) took 4.9s, 1068326 effective words/s\n",
      "2019-11-23 22:00:18,431 : INFO : EPOCH 4 - PROGRESS: at 25.57% examples, 1361608 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:19,437 : INFO : EPOCH 4 - PROGRESS: at 51.05% examples, 1342603 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:20,464 : INFO : EPOCH 4 - PROGRESS: at 71.44% examples, 1248120 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:21,465 : INFO : EPOCH 4 - PROGRESS: at 95.55% examples, 1241419 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:21,622 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 22:00:21,626 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 22:00:21,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 22:00:21,636 : INFO : EPOCH - 4 : training on 6297818 raw words (5246521 effective words) took 4.2s, 1246162 effective words/s\n",
      "2019-11-23 22:00:22,651 : INFO : EPOCH 5 - PROGRESS: at 23.31% examples, 1240248 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:23,654 : INFO : EPOCH 5 - PROGRESS: at 48.14% examples, 1267405 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:24,657 : INFO : EPOCH 5 - PROGRESS: at 70.63% examples, 1243729 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:25,663 : INFO : EPOCH 5 - PROGRESS: at 93.93% examples, 1226010 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-23 22:00:25,906 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 22:00:25,916 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 22:00:25,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 22:00:25,921 : INFO : EPOCH - 5 : training on 6297818 raw words (5245689 effective words) took 4.3s, 1225982 effective words/s\n",
      "2019-11-23 22:00:25,921 : INFO : training on a 31489090 raw words (26232213 effective words) took 22.3s, 1178928 effective words/s\n",
      "2019-11-23 22:00:25,921 : INFO : saving Word2Vec object under ./model/word2vec100CBOW, separately None\n",
      "2019-11-23 22:00:25,923 : INFO : storing np array 'vectors' to ./model/word2vec100CBOW.wv.vectors.npy\n",
      "2019-11-23 22:00:26,086 : INFO : not storing attribute vectors_norm\n",
      "2019-11-23 22:00:26,087 : INFO : storing np array 'syn1neg' to ./model/word2vec100CBOW.trainables.syn1neg.npy\n",
      "2019-11-23 22:00:26,240 : INFO : not storing attribute cum_table\n",
      "2019-11-23 22:00:26,437 : INFO : saved ./model/word2vec100CBOW\n",
      "2019-11-23 22:00:26,438 : INFO : All models are now generated, time to test !\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start to generate all the model, 2 skip gram (100/300) and 2 CBOW (100/300)\")\n",
    "models = []\n",
    "\n",
    "logging.info(\"Generating skip-gram 300...\")\n",
    "model = gensim.models.Word2Vec(dataForModel,size=300,min_count=1,sg=1)\n",
    "model.save(\"./model/word2vec300SG\")\n",
    "models.append((model,'Skip Gram 300'))\n",
    "logging.info(\"Generating skip-gram 100...\")\n",
    "model = gensim.models.Word2Vec(dataForModel,size=100,min_count=1,sg=1)\n",
    "model.save(\"./model/word2vec100SG\")\n",
    "models.append((model,'Skip Gram 100'))\n",
    "logging.info(\"Generating CBOW 300...\")\n",
    "model = gensim.models.Word2Vec(dataForModel,size=300,min_count=1)\n",
    "model.save(\"./model/word2vec300CBOW\")\n",
    "models.append((model,'CBOW 300'))\n",
    "logging.info(\"Generating CBOW 100...\")\n",
    "model = gensim.models.Word2Vec(dataForModel,size=100,min_count=1)\n",
    "model.save(\"./model/word2vec100CBOW\")\n",
    "models.append((model,'CBOW 100'))\n",
    "\n",
    "logging.info(\"All models are now generated, time to test !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:36:54,592 : INFO : loading Word2Vec object from ./model/word2vec200cbow\n",
      "2019-11-23 21:36:54,992 : INFO : loading wv recursively from ./model/word2vec200cbow.wv.* with mmap=None\n",
      "2019-11-23 21:36:54,993 : INFO : loading vectors from ./model/word2vec200cbow.wv.vectors.npy with mmap=None\n",
      "2019-11-23 21:36:55,574 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-23 21:36:55,575 : INFO : loading vocabulary recursively from ./model/word2vec200cbow.vocabulary.* with mmap=None\n",
      "2019-11-23 21:36:55,575 : INFO : loading trainables recursively from ./model/word2vec200cbow.trainables.* with mmap=None\n",
      "2019-11-23 21:36:55,576 : INFO : loading syn1neg from ./model/word2vec200cbow.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-23 21:36:56,083 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-23 21:36:56,084 : INFO : loaded ./model/word2vec200cbow\n"
     ]
    }
   ],
   "source": [
    "#mymodel = gensim.models.Word2Vec.load('./model/word2vec200cbow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bd85722d7518>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marraySimilarities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-----------------Testing word similarity for \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"-----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "#ten words to test : Euro,inanimate,Cyprus,China,dna,music,yahoo,wrong,mouth,singer,rich\n",
    "# un PDF + un readme\n",
    "\n",
    "wordList = ['Euro','inanimate','Cyprus','China','music','Yahoo','wrong','mouth','singer','rich']\n",
    "def selection_sort(x,word):\n",
    "    for i in range(len(x)):\n",
    "        swap = i + np.argmax(x[i:])\n",
    "        (x[i], x[swap]) = (x[swap], x[i])\n",
    "        (word[i], word[swap]) = (word[swap], word[i])\n",
    "    return [x,word]\n",
    "\n",
    "def getSimilarity(myWord,mymodel):\n",
    "    arrayWord = []\n",
    "    arraySimilarities = []\n",
    "    i = 0\n",
    "    for word in mymodel.wv.vocab:\n",
    "        cosineSim = mymodel.wv.similarity(myWord,word)\n",
    "        if(cosineSim > 0.2 and cosineSim != 1):\n",
    "            arraySimilarities.append((mymodel.wv.similarity(myWord,word),word))\n",
    "    arraySimilarities.sort(key=lambda tup: tup[0],reverse=True)\n",
    "    print(arraySimilarities[:10])\n",
    "    print(\"\\n\")\n",
    "for model in models:\n",
    "    logging.info(\"-----------------Testing word similarity for \"+model[1]+\"-----------------\")\n",
    "    for word in wordList:\n",
    "        print('Word used :' + word)\n",
    "        getSimilarity(word,model[0])\n",
    "#getSimilarity('Britany')\n",
    "#print(mymodel.wv.similarity('Euro','Cyprus'))\n",
    "#matrix_similarity = cosine_similarity(mymodel.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 11:31:29,759 : INFO : lang=en\n",
      "2019-11-24 11:31:29,759 : INFO : Use previously found TreeTagger directory: C:\\TreeTagger\n",
      "2019-11-24 11:31:29,760 : INFO : tagdir=C:\\TreeTagger\n",
      "2019-11-24 11:31:29,761 : INFO : tagbin=C:\\TreeTagger\\bin\\tree-tagger.exe\n",
      "2019-11-24 11:31:29,762 : INFO : tagparfile=C:\\TreeTagger\\lib\\english.par\n",
      "2019-11-24 11:31:29,763 : INFO : tagopt=-token -lemma -sgml -quiet -no-unknown\n",
      "2019-11-24 11:31:29,763 : INFO : taginencoding=utf-8\n",
      "2019-11-24 11:31:29,764 : INFO : tagoutencoding=utf-8\n",
      "2019-11-24 11:31:29,765 : INFO : taginencerr=replace\n",
      "2019-11-24 11:31:29,765 : INFO : tagoutencerr=replace\n",
      "2019-11-24 11:31:29,766 : INFO : abbrevfile=C:\\TreeTagger\\lib\\english-abbreviations\n",
      "2019-11-24 11:31:29,767 : INFO : Read 289 abbreviations from file: C:\\TreeTagger\\lib\\english-abbreviations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10056\n"
     ]
    }
   ],
   "source": [
    "import treetaggerwrapper\n",
    "def removeStopWords(pathToStopWords,pathToRead,pathToWrite):\n",
    "    stopWords = []\n",
    "    with open(pathToStopWords,encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            stopWords.append(line[:-1])\n",
    "    onlyfiles = [f for f in listdir(pathToRead) if isfile(join(pathToRead, f))]\n",
    "    for file in onlyfiles:\n",
    "        with open(pathToRead + file, \"r\",encoding='utf8') as f:\n",
    "            fwrite = open(pathToWrite + file + \".txt\", \"w\",encoding='utf8')\n",
    "            for line in f:\n",
    "                for word in line.split():\n",
    "                    if word not in stopWords:\n",
    "                        fwrite.write(word+' ')\n",
    "                fwrite.write('\\n')\n",
    "#removeStopWords('D:/Documents/Polytech/NLP/stopword-en.txt','D:/Documents/Polytech/NLP/English/','D:/Documents/Polytech/NLP/filteredTxt/')\n",
    "\n",
    "def tokenisation(pathToRead, pathToWrite):\n",
    "    onlyfiles = [f for f in listdir(pathToRead) if isfile(join(pathToRead, f))]\n",
    "    for file in onlyfiles:\n",
    "        with open(pathToRead + file, \"r\",encoding='utf8') as f:\n",
    "            fwrite = open(pathToWrite + file + \".tok\", \"w\",encoding='utf8')\n",
    "            for line in f:\n",
    "                fwrite.write(' '.join(nltk.word_tokenize(line)))\n",
    "                fwrite.write('\\n')\n",
    "#tokenisation('D:/Documents/Polytech/NLP/filteredTxt/','D:/Documents/Polytech/NLP/tok/')\n",
    "def lemmatisation(pathToRead,pathToWrite):\n",
    "    onlyfiles = [f for f in listdir(pathToRead) if isfile(join(pathToRead, f))]\n",
    "    i = 1\n",
    "    for file in onlyfiles:\n",
    "        with open(pathToRead + file, \"r\",encoding='utf8') as f:\n",
    "            fwrite = open(pathToWrite + os.path.basename(f.name) + \".lem\", \"w\",encoding='utf8')\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            for line in f:\n",
    "                for word in line.split():\n",
    "                    fwrite.write(lemmatizer.lemmatize(word) + \" \")\n",
    "                fwrite.write('\\n')\n",
    "\n",
    "#lemmatisation('D:/Documents/Polytech/NLP/tok/','D:/Documents/Polytech/NLP/leme/')\n",
    "def treeTagger(pathToRead, pathToWrite):\n",
    "    onlyfiles = [f for f in listdir(pathToRead) if isfile(join(pathToRead, f))]\n",
    "    i = 1\n",
    "    for file in onlyfiles:\n",
    "        fwrite = pathToWrite + file + \".lem\"\n",
    "        os.system(\"python D:/IDE/Anaconda/Lib/site-packages/treetaggerwrapper.py -d C:/TreeTagger/ --pipe < \" +\n",
    "                  pathToRead + file + \" > \" + fwrite)\n",
    "        print(i)\n",
    "        i += 1\n",
    "#treeTagger('D:/Documents/Polytech/NLP/leme/','D:/Documents/Polytech/NLP/tag/')\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "\n",
    "print(len(dataForFasttext))\n",
    "#tags = tagger.tag_text(dataForFasttext)\n",
    "#lemmatisation('/home/sebastien/Documents/Polytech/NLP/token/','/home/sebastien/Documents/Polytech/NLP/realLem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:07:33,252 : INFO : -----------------Testing word pairs with wordsim353 for Skip Gram 300-----------------\n",
      "2019-11-23 22:07:33,356 : INFO : Pearson correlation coefficient against D:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.4465\n",
      "2019-11-23 22:07:33,357 : INFO : Spearman rank-order correlation coefficient against D:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.4600\n",
      "2019-11-23 22:07:33,357 : INFO : Pairs with unknown words ratio: 2.3%\n",
      "2019-11-23 22:07:33,362 : INFO : -----------------Testing word pairs with wordsim353 for Skip Gram 100-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Similarities resume for Skip Gram 300\n",
      "((0.4465322056088227, 2.6050543509703988e-18), SpearmanrResult(correlation=0.45999281980500845, pvalue=1.8168672991229562e-19), 2.26628895184136)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:07:33,715 : INFO : Pearson correlation coefficient against D:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.4573\n",
      "2019-11-23 22:07:33,716 : INFO : Spearman rank-order correlation coefficient against D:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.4388\n",
      "2019-11-23 22:07:33,717 : INFO : Pairs with unknown words ratio: 2.3%\n",
      "2019-11-23 22:07:33,722 : INFO : -----------------Testing word pairs with wordsim353 for CBOW 300-----------------\n",
      "2019-11-23 22:07:33,831 : INFO : Pearson correlation coefficient against D:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.4427\n",
      "2019-11-23 22:07:33,832 : INFO : Spearman rank-order correlation coefficient against D:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.4330\n",
      "2019-11-23 22:07:33,832 : INFO : Pairs with unknown words ratio: 2.3%\n",
      "2019-11-23 22:07:33,836 : INFO : -----------------Testing word pairs with wordsim353 for CBOW 100-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Similarities resume for Skip Gram 100\n",
      "((0.4573040351610507, 3.1220745477851015e-19), SpearmanrResult(correlation=0.43880417186724185, pvalue=1.1402298964459586e-17), 2.26628895184136)\n",
      "\n",
      "\n",
      "Similarities resume for CBOW 300\n",
      "((0.4426804743688478, 5.462951542709803e-18), SpearmanrResult(correlation=0.4330235767285205, pvalue=3.3574951916403924e-17), 2.26628895184136)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:07:33,941 : INFO : Pearson correlation coefficient against D:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.4417\n",
      "2019-11-23 22:07:33,942 : INFO : Spearman rank-order correlation coefficient against D:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv: 0.4359\n",
      "2019-11-23 22:07:33,942 : INFO : Pairs with unknown words ratio: 2.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Similarities resume for CBOW 100\n",
      "((0.44166686869988636, 6.6280326569271e-18), SpearmanrResult(correlation=0.4358708423909942, pvalue=1.9774930782925625e-17), 2.26628895184136)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "for model in models:\n",
    "    logging.info(\"-----------------Testing word pairs with wordsim353 for \"+model[1]+\"-----------------\")\n",
    "    similarities = model[0].wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "    print('\\n')\n",
    "    print(\"Similarities resume for \"+ model[1])\n",
    "    print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "#import fasttext\n",
    "#model = fasttext.train_unsupervised('data/fil9')\n",
    "\n",
    "def makeAllLemmeTogether(pathToLemme):\n",
    "    onlyfiles = [f for f in listdir(pathToLemme) if isfile(join(pathToLemme, f))]\n",
    "    fwrite = open(pathToLemme + \"all.lem\", \"w\",encoding='utf8')\n",
    "    i = 1\n",
    "    for file in onlyfiles:\n",
    "        print(i)\n",
    "        i +=1\n",
    "        with open(pathToLemme + file, \"r\",encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                fwrite.write(line)\n",
    "#makeAllLemmeTogether(\"D:/Documents/Polytech/NLP/leme/\")\n",
    "\n",
    "fwrite = open('D:/Documents/Polytech/NLP/fasttext/' + \"corpus.txt\", \"w\",encoding='utf8')\n",
    "for doc in dataForFasttext:\n",
    "    for sentence in doc:\n",
    "        fwrite.write('1,\"'+sentence+'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "logging.info(\"-----------------Generating SkipGram model with FastText-----------------\")\n",
    "model = fasttext.train_unsupervised(\"D:/Documents/Polytech/NLP/fasttext/corpus.txt\", model='skipgram')\n",
    "model.save_model(\"fastTextSkipGram.bin\")\n",
    "logging.info(\"-----------------Generating CBOW model with FastText-----------------\")\n",
    "model = fasttext.train_unsupervised(\"D:/Documents/Polytech/NLP/fasttext/corpus.txt\", model='cbow')\n",
    "model.save_model(\"fastTextCBOW.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "fastModels = []\n",
    "model = FastText.load_fasttext_format('fastTextCBOW.bin')\n",
    "fastModels.append((model,'FastTextCBOW'))\n",
    "\n",
    "model = FastText.load_fasttext_format('fastTextSkipGram.bin')\n",
    "fastModels.append((model,'FastTextSkipGram'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word used :Euro\n",
      "[(0.88166994, 'EurAsEC'), (0.86298823, 'Euro-zone'), (0.8236492, 'euro'), (0.8172005, 'Eurozone'), (0.8057518, 'Europol'), (0.80007076, 'Europe.'), (0.79972875, 'Europes'), (0.7994734, 'EMU'), (0.7878887, 'Europhiles'), (0.78172755, 'Eurostat')]\n",
      "\n",
      "\n",
      "Word used :inanimate\n",
      "[(0.8888814, 'animate'), (0.8431741, 'intimate'), (0.84179956, 'inmate'), (0.8104749, 'decimate'), (0.7888785, 'glutamate'), (0.786278, 'primate'), (0.7844594, 'climate'), (0.7625569, 'mate'), (0.75348246, 'climate-related'), (0.7179187, 'ultimate')]\n",
      "\n",
      "\n",
      "Word used :Cyprus\n",
      "[(0.8428812, 'Cypriots'), (0.83738166, 'Cypriot'), (0.701944, 'Greek-Turkish'), (0.65842605, 'Catalonia'), (0.64819086, 'Ireland'), (0.6476697, 'Greece'), (0.64051557, 'Greek'), (0.63694596, 'Slovenia'), (0.6318108, 'Balkans'), (0.6303593, 'कितनी')]\n",
      "\n",
      "\n",
      "Word used :China\n",
      "[(0.96617216, 'China.'), (0.947366, 'China-US'), (0.94100684, 'Chinas'), (0.9395753, 'China-EU'), (0.88856035, 'India-China'), (0.88564396, 'US-China'), (0.8058939, 'pro-China'), (0.78511035, 'Chin'), (0.774765, 'China-Africa'), (0.7647718, 'Chinese')]\n",
      "\n",
      "\n",
      "Word used :music\n",
      "[(0.791856, 'musim'), (0.77819073, 'musical'), (0.7444466, 'art'), (0.7399065, 'Cynics'), (0.72059304, 'telltale'), (0.7121749, 'Eugenics'), (0.70930505, 'Hollywood'), (0.70835555, 'beautiful'), (0.7005452, 'telegraph'), (0.69980335, 'athletic')]\n",
      "\n",
      "\n",
      "Word used :Yahoo\n",
      "[(0.7847581, 'ग्रीनहाउस'), (0.7783735, 'YouTube'), (0.7445319, 'You'), (0.7201862, 'Welby'), (0.7054254, 'Hotel'), (0.7044099, 'CNN'), (0.7027239, 'Wenzhou'), (0.6934142, 'Your'), (0.6888195, 'Weiwei'), (0.67931294, 'Wells')]\n",
      "\n",
      "\n",
      "Word used :wrong\n",
      "[(0.89484334, 'wrong.'), (0.83376247, 'wrongful'), (0.77607656, 'wrongdoing'), (0.7554648, 'wrongdoer'), (0.6781163, 'wronged'), (0.6391027, 'mistook'), (0.63437885, 'Wrong'), (0.6240684, 'prong'), (0.6216638, 'Pessimists'), (0.6198233, 'pessimist')]\n",
      "\n",
      "\n",
      "Word used :mouth\n",
      "[(0.7876097, 'Youth'), (0.76965714, 'south'), (0.7383586, 'north-south'), (0.73109597, 'Dartmouth'), (0.730438, 'north'), (0.7228467, 'youth'), (0.70332056, 'swath'), (0.6904101, 'mouthpiece'), (0.66489935, 'South'), (0.6589646, 'South-South')]\n",
      "\n",
      "\n",
      "Word used :singer\n",
      "[(0.87481123, 'finger'), (0.87296367, 'Ischinger'), (0.8474318, 'Böhringer'), (0.84483707, 'Bollinger'), (0.8347378, 'Oettinger'), (0.8232156, 'linger'), (0.82099164, 'Eijffinger'), (0.79566675, 'Kissinger'), (0.79384094, 'Schlesinger'), (0.738724, 'Singer')]\n",
      "\n",
      "\n",
      "Word used :rich\n",
      "[(0.8508303, 'Ulrich'), (0.8366118, 'richer'), (0.83265126, 'richest'), (0.8129542, 'Zurich'), (0.7825234, 'ultra-rich'), (0.7780527, 'richly'), (0.769845, 'cash-rich'), (0.75577575, 'Gingrich'), (0.7547202, 'poorer'), (0.7519196, 'poor')]\n",
      "\n",
      "\n",
      "Word used :Euro\n",
      "[(0.7837553, 'Euroland'), (0.7425165, 'Euro-zone'), (0.7239716, 'euro'), (0.7032763, 'Eurozone'), (0.6789916, 'Europes'), (0.67686033, 'EMU'), (0.6706772, 'Euro\"1,\"'), (0.66254723, 'Europhiles'), (0.661829, 'euroland'), (0.65077597, 'Eurosystem')]\n",
      "\n",
      "\n",
      "Word used :inanimate\n",
      "[(0.8705795, 'animate'), (0.7991539, 'animated'), (0.79272795, 'animates'), (0.75086886, 'animating'), (0.7480523, 'animation'), (0.70844734, 'emasculated'), (0.70566446, 'masculine'), (0.6998342, 'obfuscate'), (0.6973466, 'subterfuge'), (0.6966213, 'originality')]\n",
      "\n",
      "\n",
      "Word used :Cyprus\n",
      "[(0.80330694, 'Cypriot'), (0.7955067, 'Cypriots'), (0.67776537, 'Cyprus\"1,\"'), (0.66087735, 'Greece'), (0.62644637, 'Greek'), (0.617623, 'Ireland'), (0.61633146, 'Iceland'), (0.6160525, 'Greek-Turkish'), (0.60581833, 'EU'), (0.6039881, 'Slovenia')]\n",
      "\n",
      "\n",
      "Word used :China\n",
      "[(0.8768017, 'China.'), (0.8698689, 'Chinas'), (0.8290578, 'China-EU'), (0.8232435, 'Chinese'), (0.8209652, 'China-US'), (0.81185627, 'India-China'), (0.80865514, 'China\"1,\"”'), (0.7949294, 'Chinese-led'), (0.7873269, 'Chin'), (0.76285803, 'Asia\"1,\"China')]\n",
      "\n",
      "\n",
      "Word used :music\n",
      "[(0.8720833, 'musical'), (0.747485, 'musician'), (0.7177958, 'musing'), (0.71310425, 'DVDs'), (0.7009865, 'art'), (0.68770593, 'theatrics'), (0.68446434, 'chat'), (0.68254966, 'DVD'), (0.6822782, 'carnival'), (0.68224835, 'audience')]\n",
      "\n",
      "\n",
      "Word used :Yahoo\n",
      "[(0.8200549, 'Facebook'), (0.8004285, 'ग्रीनहाउस'), (0.7829746, 'Skype'), (0.7777909, 'Google'), (0.7707566, 'eBay'), (0.7698741, 'Netflix'), (0.7672947, 'email'), (0.7648454, 'Weardrobe'), (0.7606382, 'Microsoft'), (0.75923413, 'YouTube')]\n",
      "\n",
      "\n",
      "Word used :wrong\n",
      "[(0.8825006, 'wrong.'), (0.78360915, 'wronged'), (0.719449, 'wrongdoer'), (0.7183269, 'wrong\"1,\"But'), (0.7056498, 'wrong-headed'), (0.69441664, 'wrongheaded'), (0.68955207, 'wrong\"1,\"And'), (0.6864213, 'wrongful'), (0.68369824, 'wrongly'), (0.66472125, 'correct')]\n",
      "\n",
      "\n",
      "Word used :mouth\n",
      "[(0.7218316, 'gaze'), (0.7168211, 'mouthpiece'), (0.7095641, 'circus'), (0.70882714, 'streetlight'), (0.69177777, 'leaflet'), (0.6873999, 'roulette'), (0.6867724, 'clot'), (0.6860518, 'stomach'), (0.68171716, 'silly'), (0.6810036, 'wallet')]\n",
      "\n",
      "\n",
      "Word used :singer\n",
      "[(0.77110046, 'poem'), (0.76246136, 'finger'), (0.7429273, 'Bollinger'), (0.7394955, 'Kissinger'), (0.7369344, 'Schlesinger'), (0.73065704, 'poet'), (0.72748077, 'sing'), (0.7224307, 'fancied'), (0.71972585, 'Oettinger'), (0.71665746, 'cricketer')]\n",
      "\n",
      "\n",
      "Word used :rich\n",
      "[(0.81076634, 'richer'), (0.7840356, 'poor'), (0.76600534, 'poorer'), (0.7624992, 'rich-world'), (0.7481451, 'richest'), (0.7364675, 'rich-country'), (0.72309256, 'super-rich'), (0.7147771, 'poorest'), (0.70850104, 'wealthy'), (0.7038696, 'wealthier')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in fastModels:\n",
    "    logging.info(\"-----------------Testing word similarity for \"+model[1]+\"-----------------\")\n",
    "    for word in wordList:\n",
    "        print('Word used :' + word)\n",
    "        getSimilarity(word,model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Similarities resume for FastTextCBOW\n",
      "((0.4289160941819007, 1.4216866666658386e-15), SpearmanrResult(correlation=0.404601579015805, pvalue=7.07934279965324e-14), 10.48158640226629)\n",
      "\n",
      "\n",
      "Similarities resume for FastTextSkipGram\n",
      "((0.5419461065390799, 1.6095150633283194e-25), SpearmanrResult(correlation=0.5429516705203242, pvalue=1.2604008241500752e-25), 10.48158640226629)\n"
     ]
    }
   ],
   "source": [
    "for model in fastModels:\n",
    "    logging.info(\"-----------------Testing word pairs with wordsim353 for \"+model[1]+\"-----------------\")\n",
    "    similarities = model[0].wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "    print('\\n')\n",
    "    print(\"Similarities resume for \"+ model[1])\n",
    "    print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
