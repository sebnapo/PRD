{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.layers import Dense, Embedding, Input, Add, Dot, Reshape, Flatten\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "from os import listdir\n",
    "import nltk\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import numpy as np\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from tensorflow.python.keras import backend as K\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 3 documents\n",
      "['the dog saw a cat', 'the dog chased the cat', 'the cat climbed a tree']\n"
     ]
    }
   ],
   "source": [
    "def read_data(pathToRead, n_lines):\n",
    "    \"\"\" Reading the zip file to extract text \"\"\"\n",
    "    docs = []\n",
    "    i = 0\n",
    "    onlyfiles = [f for f in listdir(pathToRead) if isfile(join(pathToRead, f))]\n",
    "    for filename in onlyfiles:\n",
    "        with open(pathToRead + filename, 'r', encoding='utf-8') as f:\n",
    "            for row in f:\n",
    "                file_string = nltk.word_tokenize(row)\n",
    "                # First token is the movie ID\n",
    "                docs.append(' '.join(file_string))\n",
    "                i += 1\n",
    "                if n_lines and i == n_lines:\n",
    "                    break\n",
    "    return docs\n",
    "\n",
    "docs = read_data('./data/', 3)\n",
    "print(\"Read in {} documents\".format(len(docs)))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x00000299A8CCFF88>\n"
     ]
    }
   ],
   "source": [
    "#Ici on effectue la tokenisation de notre corpus\n",
    "\n",
    "v_size = 8\n",
    "tokenizer = Tokenizer(num_words=v_size, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d17267cec393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgenerate_cooc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mcooc_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_cooc_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0msave_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datasets'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cooc_mat.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcooc_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "generate_cooc = False\n",
    "def generate_cooc_matrix(text, tokenizer, window_size, n_vocab, use_weighting=True):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    \n",
    "    cooc_mat = lil_matrix((n_vocab, n_vocab), dtype=np.float32)\n",
    "    for sequence in sequences:\n",
    "        for i, wi in zip(np.arange(window_size, len(sequence)-window_size), sequence[window_size:-window_size]):\n",
    "            context_window = sequence[i-window_size: i+window_size+1]\n",
    "            distances = np.abs(np.arange(-window_size, window_size+1))\n",
    "            distances[window_size] = 1.0\n",
    "            nom = np.ones(shape=(window_size*2 + 1,), dtype=np.float32)\n",
    "            nom[window_size] = 0.0\n",
    "            if use_weighting:\n",
    "                cooc_mat[wi, context_window] += nom/distances    # Update element\n",
    "            else:\n",
    "                cooc_mat[wi, context_window] += nom\n",
    "            print(cooc_mat[wi, context_window])\n",
    "    \n",
    "    return cooc_mat    \n",
    "\n",
    "if not generate_cooc:\n",
    "    cooc_mat = generate_cooc_matrix(docs, tokenizer, 2, v_size, False)\n",
    "    save_npz(os.path.join('datasets','cooc_mat.npz'), cooc_mat.tocsr())\n",
    "else:\n",
    "    cooc_mat = load_npz(os.path.join('datasets','cooc_mat.npz')).tolil()\n",
    "    print('Cooc matrix of type {} was loaded from disk'.format(type(cooc_mat).__name__))\n",
    "\n",
    "print(cooc_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the co-occurence matrix\n",
    "Computes the co-occurence matrix and save it to the disk can either load the existing matrix or compute a new one by changing generate_cooc variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "[1 6 7 2 3 4 5]\n",
      "[0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x299acc11c88>,\n",
       "  <matplotlib.axis.XTick at 0x299acc112c8>,\n",
       "  <matplotlib.axis.XTick at 0x299acc19e48>,\n",
       "  <matplotlib.axis.XTick at 0x299ace18908>,\n",
       "  <matplotlib.axis.XTick at 0x299ace20408>,\n",
       "  <matplotlib.axis.XTick at 0x299ace20b08>,\n",
       "  <matplotlib.axis.XTick at 0x299ace242c8>],\n",
       " <a list of 7 Text xticklabel objects>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word=\"saw\"\n",
    "assert word in tokenizer.word_index, 'Word {} is not in the tokenizer'.format(word)\n",
    "assert tokenizer.word_index[word] <= v_size, 'The word {} is an out of vocabuary word. Please try something else'.format(word)\n",
    "\n",
    "rev_word_index = dict(zip(tokenizer.word_index.values(), tokenizer.word_index.keys()))\n",
    "print(tokenizer.word_index[word])\n",
    "cooc_vec = np.array(cooc_mat.getrow(tokenizer.word_index[word]).todense()).ravel()\n",
    "print(cooc_vec)\n",
    "max_ind = np.argsort(cooc_vec)[-7:]\n",
    "print(max_ind)\n",
    "print(cooc_vec[max_ind])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.bar(np.arange(0, 7), cooc_vec[max_ind])\n",
    "plt.xticks(ticks=np.arange(0, 7), labels=[rev_word_index[i] for i in max_ind], rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition du modÃ¨le de GloVe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_model(v_size):    \n",
    "    w_i = Input(shape=(1,))\n",
    "    w_j = Input(shape=(1,))\n",
    "\n",
    "    emb_i = Flatten()(Embedding(v_size, 96, input_length=1)(w_i))\n",
    "    emb_j = Flatten()(Embedding(v_size, 96, input_length=1)(w_j))\n",
    "\n",
    "    ij_dot = Dot(axes=-1)([emb_i,emb_j])\n",
    "    \n",
    "    b_i = Flatten()(\n",
    "        Embedding(v_size, 1, input_length=1)(w_i)\n",
    "    )\n",
    "    b_j = Flatten()(\n",
    "        Embedding(v_size, 1, input_length=1)(w_j)\n",
    "    )\n",
    "\n",
    "    pred = Add()([ij_dot, b_i, b_j])\n",
    "\n",
    "    def glove_loss(y_true, y_pred):\n",
    "        return K.sum(\n",
    "            K.pow((y_true-1)/100.0, 0.75)*K.square(y_pred - K.log(y_true))\n",
    "        )\n",
    "\n",
    "    model = Model(inputs=[w_i, w_j],outputs=pred)\n",
    "    model.compile(loss=glove_loss, optimizer =Adam(lr=0.0001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 96)        768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 96)        768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 96)           0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 96)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 1)         8           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 1)         8           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1)            0           dot[0][0]                        \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,552\n",
      "Trainable params: 1,552\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = create_glove_model(v_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lancement et evaluation de GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss in epoch 0: 0.08402467394868533\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]]\n",
      "[[2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]]\n",
      "Loss in epoch 1: 0.08383174240589142\n",
      "[[2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]]\n",
      "[[2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]]\n",
      "Loss in epoch 2: 0.08361282323797543\n",
      "[[2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss in epoch 3: 0.08336362118522327\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]]\n",
      "Loss in epoch 4: 0.08315255120396614\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss in epoch 5: 0.08295487985014915\n",
      "[[2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]]\n",
      "[[2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss in epoch 6: 0.08275627096494038\n",
      "[[2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]]\n",
      "Loss in epoch 7: 0.0825162169833978\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]]\n",
      "Loss in epoch 8: 0.0823178415497144\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]]\n",
      "Loss in epoch 9: 0.08206526190042496\n"
     ]
    }
   ],
   "source": [
    "cooc_mat = load_npz(os.path.join('datasets','cooc_mat.npz'))\n",
    "batch_size =128\n",
    "copy_docs = list(docs)\n",
    "index2word = dict(zip(tokenizer.word_index.values(), tokenizer.word_index.keys()))\n",
    "\"\"\" Each epoch \"\"\"\n",
    "for ep in range(10):\n",
    "    \n",
    "    #valid_words = get_valid_words(docs, 20, tokenizer)\n",
    "    \n",
    "    random.shuffle(copy_docs)\n",
    "    losses = []\n",
    "    \"\"\" Each document (i.e. movie plot) \"\"\"\n",
    "    for doc in copy_docs:\n",
    "        \n",
    "        seq = tokenizer.texts_to_sequences([doc])[0]\n",
    "\n",
    "        \"\"\" Getting skip-gram data \"\"\"\n",
    "        # Negative samples are automatically sampled by tf loss function\n",
    "        wpairs, labels = skipgrams(\n",
    "            sequence=seq, vocabulary_size=v_size, negative_samples=0.0, shuffle=True\n",
    "        )\n",
    "        \n",
    "        if len(wpairs)==0:\n",
    "            continue\n",
    "\n",
    "        sg_in, sg_out = zip(*wpairs)\n",
    "        sg_in, sg_out = np.array(sg_in).reshape(-1,1), np.array(sg_out).reshape(-1,1)\n",
    "        x_ij = np.array(cooc_mat[sg_in[:,0], sg_out[:,0]]).reshape(-1,1) + 1\n",
    "        print(x_ij)\n",
    "        \n",
    "        assert np.all(np.array(labels)==1)\n",
    "        assert x_ij.shape[0] == sg_in.shape[0], 'X_ij {} shape does not sg_in {}'.format(x_ij.shape, sg_in.shape)\n",
    "        \"\"\" For each batch in the dataset \"\"\"\n",
    "        model.fit([sg_in, sg_out], x_ij, batch_size = batch_size, epochs=1, verbose=0)\n",
    "        l = model.evaluate([sg_in, sg_out], x_ij, batch_size=batch_size, verbose=0)\n",
    "        losses.append(l)\n",
    "    print('Loss in epoch {}: {}'.format(ep, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde du modele et des embedings localement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-c8ed77132aa6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0msave_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'datasets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\IDE\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    973\u001b[0m     \"\"\"\n\u001b[0;32m    974\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m--> 975\u001b[1;33m                       signatures, options)\n\u001b[0m\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\IDE\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[1;32m--> 112\u001b[1;33m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[1;32mD:\\IDE\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    113\u001b[0m     if (include_optimizer and model.optimizer and\n\u001b[0;32m    114\u001b[0m         not isinstance(model.optimizer, optimizers.TFOptimizer)):\n\u001b[1;32m--> 115\u001b[1;33m       \u001b[0msave_optimizer_weights_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\IDE\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_optimizer_weights_to_hdf5_group\u001b[1;34m(hdf5_group, optimizer)\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m       param_dset = weights_group.create_dataset(\n\u001b[1;32m--> 587\u001b[1;33m           name, val.shape, dtype=val.dtype)\n\u001b[0m\u001b[0;32m    588\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\IDE\\Anaconda\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\IDE\\Anaconda\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, name, obj)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[0mh5o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "def save_embeddings(model,save_dir, tok, v_size):\n",
    "    \"\"\" Saving data to disk \"\"\"\n",
    "    \n",
    "    # We need to add the 0th index to word list manually\n",
    "    word_list = [\"RESERVED\"]+[tok.index_word[w_i] for w_i in range(1,v_size)]\n",
    "    emb_w_df = None\n",
    "    for layer in model.layers:\n",
    "        if 'embedding' == layer.name or 'embedding_1' == layer.name:\n",
    "            if emb_w_df is None:\n",
    "                emb_w_df = pd.DataFrame(layer.get_weights()[0])\n",
    "            else:\n",
    "                emb_w_df += layer.get_weights()[0]\n",
    "    \n",
    "    emb_w_df.insert(0, \"word\", word_list)\n",
    "            \n",
    "    emb_w_df.to_csv(\n",
    "        os.path.join(save_dir, 'embeddings_w.csv'), index=False, header=None\n",
    "    )\n",
    "    \n",
    "save_embeddings(model, 'datasets', tokenizer, v_size)\n",
    "model.save('glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Loading the word embeddings from the disk\n",
      "Embedding shape: (8, 96)\n",
      "[[-3.12904340e-02 -2.37072600e-02  1.05774070e-02  7.84370700e-03\n",
      "  -5.70945370e-02 -3.27894350e-02 -6.20539400e-02  5.33598180e-02\n",
      "   1.32448920e-02 -5.35469900e-02  2.72486560e-02  4.04965170e-02\n",
      "   2.28268620e-02 -3.51344870e-02  1.64486800e-02 -5.75118550e-02\n",
      "  -4.54790100e-02  4.42346200e-03 -1.55392290e-02  1.27150190e-02\n",
      "   1.07014660e-02 -7.02605250e-02 -1.86458600e-02 -3.14376240e-02\n",
      "   3.45887430e-02  2.95175650e-02 -7.88096340e-02 -5.90686240e-02\n",
      "  -4.33370250e-02 -2.83415910e-02 -2.01829410e-02  2.50835160e-02\n",
      "  -3.33745400e-03 -6.95928200e-03 -2.35212100e-02  6.15609300e-02\n",
      "  -1.49804600e-02  4.49373160e-02 -3.31103300e-02  2.49636170e-03\n",
      "  -8.37332460e-02  7.46151430e-03  7.84874200e-02 -4.68258500e-02\n",
      "  -2.17478130e-02  1.82852610e-02  1.52100360e-02  5.25698800e-03\n",
      "   6.10628500e-02  1.75771490e-02  1.86252110e-02 -2.08496560e-02\n",
      "   2.71369000e-02 -2.70481930e-02  1.83598730e-02 -3.43514300e-02\n",
      "  -4.53231700e-02 -1.74217080e-02  5.63159920e-02  2.45929140e-03\n",
      "  -1.85763970e-02  1.98866990e-02 -1.15262900e-02 -3.66493950e-02\n",
      "   2.36693880e-02 -4.95598700e-02  2.24555020e-02  7.72306300e-02\n",
      "  -1.32460930e-02  9.79526000e-02 -5.61380050e-02  1.27450710e-02\n",
      "  -4.93489170e-02  1.81885730e-02  3.56658800e-02  2.45619660e-02\n",
      "  -2.04205030e-02  2.46613170e-02  5.43705300e-02 -3.66650900e-03\n",
      "   4.36185100e-02 -2.72393710e-02 -4.04630300e-02  1.46055330e-02\n",
      "   1.01953150e-02  4.75903970e-03 -3.85677330e-02 -3.44967660e-03\n",
      "  -7.23045900e-02 -6.67278300e-02 -1.80478800e-02  1.69166210e-02\n",
      "  -9.74781250e-03 -3.50042600e-02 -6.25399600e-02  3.96839680e-02]\n",
      " [ 1.30354970e-02  3.54429900e-02  4.11346100e-02  3.37205270e-02\n",
      "   1.50776900e-02 -4.35444500e-02 -4.91499230e-02 -6.14711600e-02\n",
      "  -1.33782780e-02 -7.37766250e-02 -5.09617850e-02  3.32066900e-02\n",
      "   1.12052800e-02 -5.93145080e-02 -6.82740500e-02  5.47110500e-02\n",
      "   2.83899460e-02  2.11762490e-02  2.26951650e-02 -3.07909500e-02\n",
      "   3.81314200e-04 -8.50732600e-03 -5.03211500e-02 -2.60113440e-02\n",
      "   2.28311070e-02 -1.58756880e-02 -3.37125360e-02 -1.29157575e-02\n",
      "   2.66476940e-02  5.32011000e-02 -6.27313000e-03 -2.66719940e-02\n",
      "   1.39293020e-02  2.81399530e-02  4.94111250e-02  7.39975300e-02\n",
      "  -4.64967940e-02 -4.61665130e-02  6.42534200e-02 -3.07350360e-02\n",
      "   3.95755470e-03  3.06384530e-02 -3.81086540e-02 -2.07587630e-02\n",
      "  -2.86134850e-02 -3.62399600e-02 -5.04037000e-02 -1.87673660e-02\n",
      "   1.17643850e-02 -1.86389570e-02  3.17361800e-02  3.28591580e-02\n",
      "   3.82265450e-02 -2.98891890e-02 -5.10564260e-02 -5.45366200e-02\n",
      "  -4.90696950e-02  5.12840380e-02  5.40925200e-02  3.67937350e-02\n",
      "  -6.63960200e-02  8.12550800e-02  5.51047770e-02 -3.49240000e-02\n",
      "  -3.00142900e-02  3.62271630e-02  6.96698760e-03  5.47200630e-03\n",
      "  -2.04134840e-02 -2.28814200e-02 -1.47627910e-02  1.50751710e-02\n",
      "  -7.23326800e-02 -3.45708950e-02 -2.93000150e-02 -1.20874590e-02\n",
      "   5.82333700e-02  6.67585700e-02 -4.80209850e-02 -3.95602470e-02\n",
      "   1.41362880e-02  2.06789170e-02 -4.46345500e-02 -6.73048100e-02\n",
      "   2.51178150e-02  4.53132580e-02  4.20425420e-02  8.70420400e-03\n",
      "   6.58215400e-02 -9.26720500e-02  2.30020940e-02  2.06648350e-02\n",
      "   4.15912800e-02  4.73251230e-02  6.10705540e-02  1.79942000e-02]\n",
      " [-4.18214800e-02 -4.89880900e-02  2.28807130e-02  4.03645220e-02\n",
      "  -9.43377100e-03  7.62970300e-02  2.79986410e-02  5.37015600e-02\n",
      "   3.93363830e-03  8.39449900e-02  4.70064650e-02  1.41049120e-02\n",
      "  -3.92003800e-02 -1.03615170e-02  1.26603470e-02  2.64498330e-02\n",
      "  -3.47825140e-02  2.70285270e-02 -1.14964660e-02  5.39978200e-02\n",
      "   1.92422500e-02 -2.52172020e-02  8.09829700e-03  2.93034430e-02\n",
      "   5.01397900e-02 -2.76580310e-03  6.54734700e-02  3.59179080e-02\n",
      "   2.44781800e-02 -5.88901200e-02  3.20279520e-02  5.03900350e-02\n",
      "   7.71176900e-02  1.88048220e-02 -4.21665100e-02 -6.55907300e-02\n",
      "  -5.03200670e-03 -6.22236100e-02  5.28904940e-03 -5.71128650e-02\n",
      "  -6.83557700e-02 -2.50788780e-03  3.54522100e-02 -4.08564470e-02\n",
      "   3.01501040e-02  1.87683660e-03 -1.09871510e-02  8.03377500e-02\n",
      "  -8.49902500e-02 -1.69314100e-02 -9.35893600e-02  2.85516870e-02\n",
      "  -1.25517580e-02 -6.15670420e-02 -5.00669630e-02  4.17488960e-02\n",
      "  -2.14749950e-03 -9.90655100e-03  3.98222540e-02 -4.94482740e-02\n",
      "  -2.62531470e-02 -1.96547800e-02  7.94900400e-02 -2.88838330e-02\n",
      "  -6.58680700e-02  5.37462300e-02 -6.36707340e-03 -2.58109570e-02\n",
      "  -6.00448250e-02 -8.33984800e-03 -3.86658460e-02  4.62418830e-02\n",
      "  -2.05521210e-02 -1.70516910e-02 -1.50976960e-02  1.39949540e-02\n",
      "   1.80433040e-02  1.58875060e-02 -8.50773700e-02 -7.31060000e-03\n",
      "   3.55867100e-02  1.21283040e-02  6.68476300e-02  4.32326230e-02\n",
      "  -4.26368940e-02  2.33951300e-02 -6.35389600e-02  5.62219620e-02\n",
      "   6.35740300e-02  1.68937330e-02 -1.59930360e-02 -5.41852500e-02\n",
      "   3.24074200e-02  2.65687740e-02  1.60129200e-02 -2.53510160e-02]\n",
      " [ 3.96261000e-03 -1.21012030e-02 -7.50916600e-02 -2.23898300e-02\n",
      "  -6.31156200e-02  1.80566890e-02  4.27046800e-04  6.04041670e-03\n",
      "   1.86687710e-03 -3.63531040e-02 -4.69177600e-02 -4.62678040e-02\n",
      "  -6.78923600e-02 -8.14932660e-02 -7.19208800e-03 -3.06497270e-02\n",
      "   6.79268300e-02 -3.40222380e-03 -2.16305570e-02 -2.69236840e-02\n",
      "  -7.25063100e-02 -2.11981350e-02  5.18614650e-02  3.79411130e-03\n",
      "  -3.44010220e-02  4.66598570e-03  1.57857070e-02 -8.95730800e-03\n",
      "  -4.77438870e-02  6.92324200e-02  2.30830420e-02  9.41057500e-02\n",
      "   3.66901570e-02 -2.94757190e-02  1.83110300e-02 -3.20967730e-02\n",
      "   1.30507555e-02  2.91134040e-03  5.11548970e-02 -1.62170980e-02\n",
      "   2.14184000e-02  3.33505870e-02 -7.55666500e-02  2.52738560e-02\n",
      "  -4.95260000e-03 -8.54576600e-03  2.13328490e-02  1.78404190e-02\n",
      "  -1.61297320e-02 -1.24577675e-02  4.30923330e-03 -3.04935390e-02\n",
      "  -4.59603400e-03  2.42221560e-02  2.23387030e-02  1.47243420e-02\n",
      "  -8.79684200e-02 -1.13026530e-02 -4.74501770e-03  4.98486900e-02\n",
      "  -6.88013360e-02  7.12432150e-03 -1.33162360e-03  4.89139970e-02\n",
      "  -1.40149890e-03 -2.37774480e-03  3.43056500e-02  1.32417380e-02\n",
      "  -4.93816170e-02  4.59188670e-02 -1.95673260e-02 -4.03317770e-02\n",
      "  -3.91049240e-02 -6.34057450e-02 -3.98935800e-02  7.41612200e-05\n",
      "  -2.47518200e-02  1.45551470e-02 -6.05870800e-03  6.49954750e-03\n",
      "  -5.23699750e-02  1.99204350e-02 -3.72151200e-02  6.82567660e-02\n",
      "   7.16053000e-02  2.04696800e-02  1.58360300e-02  6.49346560e-02\n",
      "  -2.69500130e-02  1.03000680e-02 -5.22896870e-02 -2.65030150e-02\n",
      "   3.16879300e-02 -6.85475250e-02 -4.44329600e-02  2.85180250e-02]\n",
      " [ 1.77214830e-02  9.10034300e-02  7.23230800e-02  9.77011000e-03\n",
      "   1.83756280e-02 -3.80646440e-04  5.13793900e-03 -6.33951950e-02\n",
      "   9.04759300e-02 -7.29449100e-05  3.76531600e-02 -6.55359200e-02\n",
      "   7.94964300e-03 -6.32269900e-02 -8.17417350e-03  2.97979530e-02\n",
      "  -3.29051700e-02  4.53937430e-03 -8.25195000e-03 -3.84416060e-03\n",
      "   1.31629050e-02  2.51206660e-02  1.82250550e-02 -4.96330500e-02\n",
      "   2.12539900e-02 -6.12175100e-02  3.48426000e-02  7.65318650e-02\n",
      "  -5.72228250e-02  7.69105370e-03 -6.03458730e-02 -3.95111330e-02\n",
      "   1.06130170e-02  7.39081650e-02 -4.62883400e-02 -2.35792750e-02\n",
      "   5.26600400e-02  1.52986810e-02  7.85454660e-02 -3.96137000e-02\n",
      "   3.19395300e-02  7.61319800e-03  8.64615700e-03  8.50814100e-02\n",
      "  -1.90512050e-02  4.01813130e-02  6.54650500e-03  6.24588880e-02\n",
      "  -5.84490600e-02 -2.63888610e-02  1.55964910e-03 -8.09350760e-02\n",
      "  -1.96235350e-02  2.02368310e-02  2.49802640e-02  1.00828560e-01\n",
      "   4.02781700e-02  6.70641200e-02  7.10511360e-02  1.62552950e-03\n",
      "  -3.73315070e-04  1.76069700e-03 -7.24024600e-02 -1.28026530e-02\n",
      "   5.49496260e-02 -7.33013900e-02 -7.95247400e-02  1.15664750e-02\n",
      "  -6.40506600e-02  3.13161350e-02 -7.42565650e-03  2.17083860e-02\n",
      "  -5.14425260e-02  1.75617300e-02  3.83676960e-02 -2.06220520e-03\n",
      "   2.18891760e-02  7.33154340e-02  5.87207820e-02 -2.55278800e-02\n",
      "   1.38731880e-02 -3.53703830e-02 -6.64791000e-02 -3.34667080e-02\n",
      "  -1.42398100e-03  1.07510565e-02 -3.64513400e-02 -2.24688500e-02\n",
      "  -5.37867470e-02 -1.38009600e-02  7.92807900e-03  4.93241470e-03\n",
      "   6.54005560e-02  4.41272040e-02 -2.94772780e-02  7.33994700e-02]\n",
      " [-8.96691500e-03  2.88439170e-02  8.81787300e-03  7.06229950e-02\n",
      "  -1.67897230e-02  5.12552080e-02 -2.96104700e-03  5.32138160e-02\n",
      "   3.33010550e-02  3.30715070e-03 -7.89924100e-02  1.05752900e-02\n",
      "  -8.07629800e-02 -3.32870070e-02  5.04982900e-02 -4.23071900e-02\n",
      "  -6.62749300e-02  5.37808500e-02 -6.19747040e-02 -5.81528700e-02\n",
      "   2.94017030e-02  4.08711400e-02 -1.66742410e-02  1.47897270e-02\n",
      "  -2.35815350e-03  7.28879100e-04  2.99349870e-02 -7.83477500e-02\n",
      "  -9.40689440e-02 -1.54687430e-02  2.96230500e-02  3.74121550e-02\n",
      "   3.88814200e-02 -3.85885830e-02 -4.40931600e-02  9.11486500e-02\n",
      "   2.45115220e-02 -1.35206330e-02  8.72723760e-05  3.98622940e-02\n",
      "   7.64862800e-03  7.27358500e-02  5.31469300e-03  4.38412500e-02\n",
      "  -4.14625370e-02 -4.01151400e-02 -1.23538080e-03 -2.75997300e-02\n",
      "  -5.47410770e-02  3.79283170e-03 -4.74308730e-02  2.34787040e-02\n",
      "  -7.17123750e-02  5.49599570e-03  4.29675430e-02  3.83428820e-02\n",
      "   8.17562700e-02  3.28061200e-02  1.07375530e-03 -3.93734270e-03\n",
      "  -2.36003450e-02 -3.27502500e-02 -9.33134600e-03  3.95897300e-02\n",
      "  -5.85669100e-02  1.91022600e-02  6.11787500e-02  8.61850500e-02\n",
      "   7.11529200e-02  5.03579800e-02  3.35910600e-02 -5.89119420e-02\n",
      "   3.16109470e-02  4.24482200e-02 -1.31847040e-02  2.77911360e-03\n",
      "  -2.21254860e-02  6.83644700e-02 -1.15651170e-02  6.39213900e-02\n",
      "   2.73073450e-02 -6.04144200e-02 -9.69283900e-02 -1.45947600e-02\n",
      "   2.80647370e-02  8.05665400e-02 -2.93644520e-02 -3.93071900e-02\n",
      "   6.77378480e-03 -4.69529070e-02 -4.59782560e-02  1.56902370e-02\n",
      "   1.25641890e-02  4.64017170e-02  1.42890250e-02 -2.05818660e-02]\n",
      " [ 2.11419610e-02  4.17310500e-02 -1.27593875e-02  5.71245700e-02\n",
      "   3.77947500e-02 -6.50210200e-02 -1.63153150e-02  4.69242800e-03\n",
      "   1.34887230e-03 -2.68195830e-02 -4.01992700e-02 -3.53381400e-02\n",
      "   5.70595670e-02 -6.41816850e-02  1.51796120e-02  3.50926560e-02\n",
      "  -9.00024600e-02  5.22451480e-02 -3.55833880e-02 -5.37896830e-02\n",
      "  -4.72671000e-03  6.46586600e-02  7.46762160e-02 -3.91756500e-02\n",
      "   3.20050000e-02 -4.97172470e-02 -8.40860000e-02  5.35729640e-02\n",
      "   1.19271260e-02  5.46221660e-02  1.37578700e-04 -7.21098800e-02\n",
      "   1.65761840e-02  4.19985540e-02 -4.92013200e-02 -2.74068200e-02\n",
      "  -3.97081380e-02 -9.47699840e-02 -1.18326810e-02  2.48516200e-02\n",
      "  -2.57841370e-02 -4.91168350e-02 -3.48058120e-02 -1.92392900e-02\n",
      "  -3.30236550e-02 -2.78653690e-02  5.65231400e-03 -2.70251560e-03\n",
      "  -2.62063980e-02 -1.71374200e-02 -3.29655040e-02  8.68603900e-02\n",
      "   1.50060500e-02  1.16215910e-02 -4.43895240e-02  3.97485800e-02\n",
      "   4.61631570e-02  6.84638600e-02  1.04380290e-02  2.87053290e-02\n",
      "   9.65067300e-02 -5.12682570e-02 -6.54072300e-02 -4.14984340e-02\n",
      "  -1.05392150e-02 -4.39598200e-02 -5.74784130e-02  7.31216560e-03\n",
      "  -1.94427970e-02  6.98444400e-02 -1.99079870e-02 -2.80466820e-02\n",
      "  -6.52821400e-02 -8.94584100e-03  3.97507730e-02 -8.66172650e-03\n",
      "   5.80040500e-02  8.54478600e-03 -3.78937420e-02 -3.64643700e-02\n",
      "   2.27095200e-03 -5.14436440e-02 -7.05528450e-03 -4.05684700e-02\n",
      "   7.53134500e-03 -5.01685960e-03 -5.94076600e-02  4.41969700e-02\n",
      "   9.23283400e-03  1.98207780e-02  5.28721960e-02  2.77270040e-02\n",
      "   1.25904010e-02 -4.97926200e-03 -8.57468840e-02  4.66464700e-02]\n",
      " [ 3.85305430e-02  1.44284535e-02  1.78909790e-02  9.08534500e-03\n",
      "   3.73875570e-02  6.10160900e-02  2.40511370e-02  3.76935600e-02\n",
      "   5.80436850e-02 -2.47180000e-02 -1.60013510e-02 -4.89580330e-02\n",
      "   9.68836250e-05  1.49100800e-02  2.33499180e-02  5.38333240e-02\n",
      "   3.94425200e-02 -3.48449540e-02  3.81126480e-02  3.90033570e-02\n",
      "  -3.78531000e-02 -4.78845870e-02  4.16330960e-02  4.75581850e-02\n",
      "  -2.67650900e-02  9.56467900e-03  6.49419600e-03 -3.64173130e-02\n",
      "   4.47446960e-02  6.24426360e-03 -8.52836500e-03 -7.91535900e-03\n",
      "  -3.58032620e-02 -4.99570930e-02 -5.01137700e-02 -5.85116630e-02\n",
      "   3.45978900e-04  5.79951000e-02  1.02856350e-02 -4.91319220e-03\n",
      "  -8.73174700e-03 -5.02595680e-02 -3.68990400e-02 -7.19891640e-02\n",
      "  -7.34205250e-02 -1.91227940e-02  2.58658660e-02 -3.25030240e-02\n",
      "  -4.39399340e-03 -2.60695260e-02 -8.74834360e-02  3.36976120e-03\n",
      "  -2.34022620e-02  6.09193000e-03 -2.85220100e-02  6.69077600e-02\n",
      "   6.58933730e-03 -2.55395550e-02  3.55726300e-02  1.73866730e-02\n",
      "  -3.03267430e-02 -1.90268740e-02  5.07620870e-02  3.68297850e-03\n",
      "  -3.83138660e-03 -7.76874600e-02  7.75356900e-02 -4.15486840e-02\n",
      "   1.39622390e-02  1.46723050e-02  6.02633950e-02  1.36344950e-02\n",
      "  -9.16780200e-02 -2.34150980e-02 -7.64165100e-04 -7.02110750e-02\n",
      "  -3.04226400e-02 -1.63554800e-02  1.60590190e-02 -7.95814400e-03\n",
      "   5.90526540e-02 -6.30628840e-02 -5.88545600e-04 -1.69057400e-04\n",
      "  -4.71239900e-03 -8.39374200e-03 -2.53405090e-02  4.41231360e-02\n",
      "   4.49962540e-02  2.57813320e-02 -1.03590580e-02  2.37143860e-02\n",
      "   8.41565700e-02 -1.18141845e-02  1.95604190e-02  1.03414100e-02]]\n",
      "['RESERVED' 'UNK' 'the' 'cat' 'dog' 'a' 'saw' 'chased']\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n",
      "the: chased, a, saw, dog, UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\IDE\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "def get_valid_words(docs, size, tok):\n",
    "    \"\"\" Get a random set of words to check the embeddings \"\"\"\n",
    "    np.random.seed(120)\n",
    "    valid_docs = np.random.choice(docs, size=size//2)\n",
    "    valid_words = []\n",
    "    for doc in valid_docs:\n",
    "        np.random.seed(100)\n",
    "        words = np.random.choice(tok.texts_to_sequences([doc])[0],size=2)\n",
    "        valid_words.extend(words)\n",
    "        \n",
    "    return valid_words\n",
    "        \n",
    "\n",
    "valid_words = get_valid_words(docs, 10, tokenizer)\n",
    "print(valid_words)\n",
    "def load_embeddings(filename):\n",
    "    print('Loading the word embeddings from the disk')\n",
    "    embed_df = pd.read_csv(filename, index_col=False, header=None)\n",
    "    embed_df = embed_df.set_index(0)\n",
    "    print('Embedding shape: {}'.format(embed_df.shape))\n",
    "    embed_mat = embed_df.values\n",
    "    words = embed_df.index.values\n",
    "    return embed_mat, words\n",
    "\n",
    "def get_cosine_sim(emb, valid_words, top_k):\n",
    "    norm = np.sqrt(np.sum(emb**2,axis=1,keepdims=True))\n",
    "    norm_emb = emb/norm\n",
    "    in_emb = norm_emb[valid_words,:]\n",
    "    similarity = np.dot(in_emb, np.transpose(norm_emb))\n",
    "    sorted_ind = np.argsort(-similarity, axis=1)[:,1:top_k+1]\n",
    "    return sorted_ind, valid_words\n",
    "\n",
    "embed_mat, words = load_embeddings(os.path.join('datasets','embeddings_w.csv'))\n",
    "print(embed_mat)\n",
    "print(words)\n",
    "best_ids, wids = get_cosine_sim(embed_mat, valid_words, 5)\n",
    "pd.Series(words).to_csv(os.path.join('datasets','index2word.csv'))\n",
    "for w, bi in zip(wids, best_ids):\n",
    "    print(\"{}: {}\".format(words[w], ', '.join(words[bii] for bii in bi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
